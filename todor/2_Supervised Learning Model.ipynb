{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from .utils import  *\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2: Supervised Learning Model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and clean the train and test dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('data/mail_train.csv',sep=';')\n",
    "mailout_test = pd.read_csv('data/mail_test.csv',sep=';')\n",
    "\n",
    "\n",
    "#most of the features has less than 30% missing values similar to azdias and customers datasets\n",
    "sns.distplot(mailout_train.isnull().sum()/len(mailout_train))\n",
    "sns.distplot(mailout_test.isnull().sum()/len(mailout_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#process load data\n",
    "attributes = pd.read_excel('data/attributes.xlsx', engine='openpyxl', skiprows = 1)\n",
    "\n",
    "#train data\n",
    "mailout_train = unknown_unify(mailout_train, attributes)\n",
    "X = feature_transform(mailout_train)\n",
    "\n",
    "#test data\n",
    "mailout_test = unknown_unify(mailout_test, attributes)\n",
    "X_sub = feature_transform(mailout_test)\n",
    "\n",
    "#----!\n",
    "\n",
    "y = X['RESPONSE']\n",
    "X.drop(['RESPONSE'], axis = 1, inplace = True)\n",
    "\n",
    "print(X.shape, X_sub.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.barplot(x = y.value_counts().index, y = y.value_counts())\n",
    "plt.xlabel('RESPONSE')\n",
    "plt.ylabel('Count')\n",
    "print(y.value_counts()/y.count())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Test the initial models for overall performance based on the competition metri ROC_AUC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create list of models with default hyerparameters using models with capability for probabilistic prediction\n",
    "model_list = {'LR':LogisticRegression(), 'RF' : RandomForestClassifier(),'LGBM':  LGBMClassifier(), 'XGB':  XGBClassifier()}\n",
    "\n",
    "#loop through the models and evaluate basic performance\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "for keys in model_list:\n",
    "    #plot series\n",
    "    pipeline = Pipeline([('impute', SimpleImputer(strategy= 'constant', fill_value = -1)),('scale', StandardScaler()),  ('clf', model_list[keys])])\n",
    "    plot_roc(pipeline,X, y, keys)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results:\n",
    "\n",
    "It's clear that with the default parameters LGBMClassifier gives the highest results. After some parameter tuning and submission to the competition actually XGB gave higher scores so I will continue with BayesianOptimization of the algorithm\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('impute', SimpleImputer(strategy= 'constant', fill_value = -1)),\n",
    "                     ('clf', XGBClassifier())\n",
    "                    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining machine learning pipeline:\n",
    "xgbc_ml_pipe_bayes = pipeline\n",
    "\n",
    "# Setting parameters to be tested:\n",
    "bayes_search_space = {'clf__booster': Categorical(['gbtree', 'dart']),\n",
    "                      'clf__learning_rate': Real(0.01, 0.3),\n",
    "                      'clf__gamma': Integer(0, 100),\n",
    "                      'clf__min_child_weight': Integer(0, 10),\n",
    "                      'clf__reg_lambda': Integer(1, 100),\n",
    "                      'clf__reg_alpha': Integer(0, 100),\n",
    "                      'clf__tree_method': Categorical(['auto', 'hist']),\n",
    "                      'clf__max_depth': Integer(2, 7)\n",
    "}\n",
    "\n",
    "# Defining function to display scores:\n",
    "def show_score(optim_result):\n",
    "    '''\n",
    "    It shows iteration scores during Bayesian Optimization\n",
    "    '''\n",
    "    # Computing score:\n",
    "    score = xgbc_bayes_clf.best_score_\n",
    "    print('Best ROC_AUC Score:{}'.format(score))\n",
    "\n",
    "    # Early stop:\n",
    "    if score >= 0.81:\n",
    "        print('At least 0.81 ROC_AUC score achieved!')\n",
    "\n",
    "        return True\n",
    "\n",
    "# Grid search + ML pipleine:\n",
    "xgbc_bayes_clf = BayesSearchCV(xgbc_ml_pipe_bayes, bayes_search_space, scoring = 'roc_auc', cv = 5, verbose = 2,\n",
    "                                   n_iter = 10\n",
    ")\n",
    "\n",
    "# Training model:\n",
    "xgbc_bayes_clf.fit(X, y, callback = show_score)# Defining machine learning pipeline:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([('impute', SimpleImputer(strategy= 'constant', fill_value = -1)),\n",
    "#                      ('clf', XGBClassifier())\n",
    "#                     ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bayesian Optimization of XGB hyperparameters.\n",
    "\n",
    "Using 5 fold StratifiedKFold to evaluate each step. Here I use only 10 steps for example and try to keep the notebook as clean as possible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ITERATIONS = 10\n",
    "# SEED = 42\n",
    "#\n",
    "# bayes_cv_tuner_xg = BayesSearchCV(\n",
    "#     estimator = pipeline,\n",
    "#     search_spaces = {\n",
    "#         'clf__learning_rate': (0.001, 0.9, 'log-uniform'),\n",
    "#         'clf__max_depth': (2, 10),\n",
    "#         'clf__min_child_weight': (1, 10),\n",
    "#         'clf__gamma': (0.0, 1.0, 'uniform'),\n",
    "#         'clf__subsample': (0.5, 1.0, 'uniform'),\n",
    "#         'clf__colsample_bytree': (0.5, 1.0, 'uniform'),\n",
    "#         'clf__reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
    "#         'clf__n_estimators': (50, 500),\n",
    "#         'clf__scale_pos_weight': (1,90)\n",
    "#\n",
    "#     },\n",
    "#     scoring = 'roc_auc',\n",
    "#     cv = StratifiedKFold(\n",
    "#         n_splits=5,\n",
    "#         shuffle=True,\n",
    "#         random_state= SEED\n",
    "#     ),\n",
    "#     n_jobs = -1,\n",
    "#     n_iter = ITERATIONS,\n",
    "#     verbose = 0,\n",
    "#     refit = True,\n",
    "#     random_state = np.random.RandomState(50)\n",
    "# )\n",
    "#\n",
    "# def status_print(optim_result):\n",
    "#     \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n",
    "#\n",
    "#     # Get all the models tested so far in DataFrame format },\n",
    "#     all_models = pd.DataFrame(bayes_cv_tuner_xg.cv_results_)\n",
    "#\n",
    "#     # Get current parameters and the best parameters\n",
    "#     best_params = pd.Series(bayes_cv_tuner_xg.best_params_)\n",
    "#\n",
    "#     print('Model #{}\\nBest ROC-AUC: {}\\nBest params: {}\\n'.format(\n",
    "#         len(all_models),\n",
    "#         np.round(bayes_cv_tuner_xg.best_score_, 4),\n",
    "#         bayes_cv_tuner_xg.best_params_\n",
    "#     ))\n",
    "#\n",
    "#     # Save all model results\n",
    "#     clf_name = bayes_cv_tuner_xg.estimator.__class__.__name__\n",
    "#     all_models.to_csv(clf_name+\"_cv_results.csv\")\n",
    "#\n",
    "# %time\n",
    "# result_xg = bayes_cv_tuner_xg.fit(X, y, callback=status_print)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#optimized algorithm\n",
    "parameters= dict([('colsample_bytree', 0.6888348010130712), ('gamma', 0.43702345115978325), ('learning_rate', 0.0030871248366675184), ('max_depth', 3), ('min_child_weight', 5), ('n_estimators', 376), ('reg_alpha', 0.9682055105813826), ('scale_pos_weight', 86), ('subsample', 0.9162583886152913)])\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('impute', SimpleImputer(strategy= 'constant', fill_value = -1)),\n",
    "                     ('clf', XGBClassifier(**parameters))\n",
    "                    ])\n",
    "\n",
    "fig\n",
    "fig1 = plt.figure(figsize = (10,10))\n",
    "plot_roc(pipeline,X, y, 'XGB')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving X:\n",
    "filename = 'X.pkl'\n",
    "pickle.dump(X, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving X_sub:\n",
    "filename = 'X_sub.pkl'\n",
    "pickle.dump(X_sub, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving y:\n",
    "filename = 'y.pkl'\n",
    "pickle.dump(y, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving mailout_test:\n",
    "filename = 'mailout_test.pkl'\n",
    "pickle.dump(mailout_test, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving parameters:\n",
    "filename = 'parameters.pkl'\n",
    "pickle.dump(parameters, open(filename, 'wb'))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}