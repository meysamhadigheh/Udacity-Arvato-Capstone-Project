{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import plotly.express as px\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from .utils import  *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X=pickle.load(open(\"X.pkl\", \"rb\"))\n",
    "X_sub=pickle.load(open(\"X_sub.pkl\", \"rb\"))\n",
    "parameters=pickle.load(open(\"parameters.pkl\", \"rb\"))\n",
    "mailout_test=pickle.load(open(\"mailout_test.pkl\", \"rb\"))\n",
    "y=pickle.load(open(\"y.pkl\", \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3: Kaggle Competition (4th position in the leaderboard)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### At the end highest result was achieved after using the model with the first 200 features based on the feature importances of the XGB. Maybe it's better to keep the slightly lower score based on all features because the test at Kaggle is done only at 30% of the test data. Using all the features should generalize better and probably will give better results on new data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_features = 200\n",
    "\n",
    "#get the feature importances DataFrame and plot the barchart\n",
    "model_fi = XGBClassifier( verbose = False).fit( X, y)\n",
    "fi = pd.DataFrame({'features':X.columns, 'importance' : model_fi.feature_importances_/ model_fi.feature_importances_.sum()}).sort_values(['importance'], ascending = False)\n",
    "fig = px.bar(x = fi['features'], y = fi['importance'])\n",
    "fig.show()\n",
    "\n",
    "#reduce the ftrain and test data based on the important features\n",
    "X_reduced = X[fi['features'].head(num_features).to_list()]\n",
    "X_sub_reduced = X_sub[fi['features'].head(num_features).to_list()]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#use the dictionaty with optimized parameters result of the BayesianOptimization\n",
    "model = XGBClassifier(**parameters)\n",
    "\n",
    "#predefined funciton for trainings and exporting the final model\n",
    "predict_sub(X_reduced, y, model, X_sub_reduced,mailout_test)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}