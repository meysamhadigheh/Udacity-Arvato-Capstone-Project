{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (1.4.2)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from xgboost) (1.5.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from xgboost) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (1.2.0)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from bayesian-optimization) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from bayesian-optimization) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from bayesian-optimization) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (3.2.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from lightgbm) (1.5.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from lightgbm) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from lightgbm) (0.23.2)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from lightgbm) (0.36.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize==0.8.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (0.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-optimize==0.8.1) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-optimize==0.8.1) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-optimize==0.8.1) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-optimize==0.8.1) (1.5.3)\n",
      "Requirement already satisfied: pyaml>=16.9 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-optimize==0.8.1) (21.8.3)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from pyaml>=16.9->scikit-optimize==0.8.1) (5.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-learn>=0.20.0->scikit-optimize==0.8.1) (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-optimize==0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==0.23.2 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-learn==0.23.2) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-learn==0.23.2) (1.5.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-learn==0.23.2) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-learn==0.23.2) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn==0.23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from utils import  *\n",
    "from skopt import BayesSearchCV\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean the train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (18,19) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "mailout_train = pd.read_csv('../../data/mail_train.csv',sep=';')\n",
    "mailout_test = pd.read_csv('../../data/mail_test.csv',sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PRAEGENDE_JUGENDJAHRE_DECADE feature\n",
      "Creating PRAEGENDE_JUGENDJAHRE_MOVEMENT feature\n",
      "Creating PRAEGENDE_JUGENDJAHRE_DECADE feature\n",
      "Creating PRAEGENDE_JUGENDJAHRE_MOVEMENT feature\n",
      "(42962, 364) (42833, 364)\n"
     ]
    }
   ],
   "source": [
    "#process load data\n",
    "attributes = pd.read_excel('../../data/attributes.xlsx', engine='openpyxl', skiprows = 1)\n",
    "\n",
    "#train data\n",
    "mailout_train = unknown_unify(mailout_train, attributes)\n",
    "X = feature_transform(mailout_train)\n",
    "\n",
    "#test data\n",
    "mailout_test = unknown_unify(mailout_test, attributes)\n",
    "X_sub = feature_transform(mailout_test)\n",
    "\n",
    "#----!\n",
    "\n",
    "y = X['RESPONSE']\n",
    "X.drop(['RESPONSE'], axis = 1, inplace = True)\n",
    "\n",
    "print(X.shape, X_sub.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create prediction csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sub1(X_train, y_train, model, X_test, mailout_test,file_name):\n",
    "    '''\n",
    "    Trains the final model and exports Submission.xls file for uploading to the Kaggle competition\n",
    "    \n",
    "    Args: \n",
    "        X_train: DataFrame() - Train data \n",
    "        y_train: DataFrame() - Train data labels\n",
    "        model: sklearn.model - Final model to train\n",
    "        X_test: string - Test data to predict for the Kaggle competition.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_sub = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    submission = pd.DataFrame({'LNR': mailout_test['LNR'], 'RESPONSE': y_sub})\n",
    "    submission.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bayesian with simple tunning test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# SETTINGS - CHANGE THESE TO GET SOMETHING MEANINGFUL\n",
    "ITERATIONS = 10 # 1000\n",
    "TRAINING_SIZE = 100000 # 20000000\n",
    "TEST_SIZE = 25000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "bayes_cv_tuner = BayesSearchCV(\n",
    "    estimator = xgb.XGBClassifier(\n",
    "        n_jobs = 1,\n",
    "        objective = 'binary:logistic',\n",
    "        eval_metric = 'auc',\n",
    "        silent=1,\n",
    "        tree_method='approx',\n",
    "        subsample=0.8,\n",
    "        eta=0.1\n",
    "        \n",
    "    ),\n",
    "    search_spaces = {\n",
    "        'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "        'max_depth': (3.0, 10.0),\n",
    "        'gamma': (1e-9, 1.0, 'log-uniform'),\n",
    "        'n_estimators': (100.0, 120.0)\n",
    "        \n",
    "        \n",
    "    },    \n",
    "    scoring = 'roc_auc',\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=3,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs = 3,\n",
    "    n_iter = ITERATIONS,   \n",
    "    verbose = 0,\n",
    "    refit = True,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "def status_print(optim_result):\n",
    "    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n",
    "    \n",
    "    # Get all the models tested so far in DataFrame format\n",
    "    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n",
    "    \n",
    "    # Get current parameters and the best parameters    \n",
    "    best_params = pd.Series(bayes_cv_tuner.best_params_)\n",
    "    print('Model #{}\\nBest ROC-AUC: {}\\nBest params: {}\\n'.format(\n",
    "        len(all_models),\n",
    "        np.round(bayes_cv_tuner.best_score_, 4),\n",
    "        bayes_cv_tuner.best_params_\n",
    "    ))\n",
    "    \n",
    "    # Save all model results\n",
    "    clf_name = bayes_cv_tuner.estimator.__class__.__name__\n",
    "    all_models.to_csv(clf_name+\"_cv_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "result = bayes_cv_tuner.fit(X.values, y.values, callback=status_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('gamma', 0.020491035004385), ('learning_rate', 0.022066991249460103), ('max_depth', 7), ('n_estimators', 116)])\n",
      "Best ROC_AUC score: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Checking best parameters:\n",
    "print(bayes_cv_tuner.best_params_)\n",
    "\n",
    "# Checking best score:\n",
    "print('Best ROC_AUC score: {:.2f}'.format(bayes_cv_tuner.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters= dict([('gamma', 0.020491035004385), ('learning_rate', 0.022066991249460103), ('max_depth', 7), ('n_estimators', 116)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:38:00] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"verbose\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:38:00] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "num_features = 366\n",
    "\n",
    "#get the feature importances DataFrame and plot the barchart\n",
    "model_fi = XGBClassifier( verbose = False).fit( X, y)\n",
    "fi = pd.DataFrame({'features':X.columns, 'importance' : model_fi.feature_importances_/ model_fi.feature_importances_.sum()}).sort_values(['importance'], ascending = False)\n",
    "\n",
    "\n",
    "#reduce the ftrain and test data based on the important features\n",
    "X_reduced = X[fi['features'].head(num_features).to_list()]\n",
    "X_sub_reduced = X_sub[fi['features'].head(num_features).to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:38:29] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "#use the dictionaty with optimized parameters result of the BayesianOptimization\n",
    "model = XGBClassifier(**parameters)\n",
    "\n",
    "#predefined funciton for trainings and exporting the final model\n",
    "predict_sub(X_reduced, y, model, X_sub_reduced,mailout_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pbounds = {\n",
    "    'learning_rate': (0.01, 1.0),\n",
    "    'n_estimators': (100, 1000),\n",
    "    'max_depth': (3,10),\n",
    "    'subsample': (1.0, 1.0),  # Change for big datasets\n",
    "    'colsample': (1.0, 1.0),  # Change for datasets with lots of features\n",
    "    'gamma': (0, 5)}\n",
    "\n",
    "def xgboost_hyper_param(learning_rate,\n",
    "                        n_estimators,\n",
    "                        max_depth,\n",
    "                        subsample,\n",
    "                        colsample,\n",
    "                        gamma):\n",
    "\n",
    "    max_depth = int(max_depth)\n",
    "    n_estimators = int(n_estimators)\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        gamma=gamma)\n",
    "    return np.mean(cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc'))\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=xgboost_hyper_param,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.maximize(n_iter=5, init_points=8, acq='ei')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
