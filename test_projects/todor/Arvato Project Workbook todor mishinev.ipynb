{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pip install xgboost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pip install bayesian-optimization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pip install lightgbm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pip install scikit_optimize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import plotly.express as px\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "# from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from lightgbm import LGBMClassifier\n",
    "from .utils import  *\n",
    "# import skopt\n",
    "# from skopt import BayesSearchCV\n",
    "# class BayesSearchCV(BayesSearchCV):\n",
    "#     def _run_search(self, x): raise BaseException('Use newer skopt')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['PRODUCT_GROUP' 'CUSTOMER_GROUP' 'ONLINE_PURCHASE'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-ed36be3db88c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;31m#drop 3 of the customer columns which are missing in the global population dataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0mcustomers_clean\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcustomers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'PRODUCT_GROUP'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'CUSTOMER_GROUP'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'ONLINE_PURCHASE'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minplace\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36mdrop\u001B[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[1;32m   4306\u001B[0m                 \u001B[0mweight\u001B[0m  \u001B[0;36m1.0\u001B[0m     \u001B[0;36m0.8\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4307\u001B[0m         \"\"\"\n\u001B[0;32m-> 4308\u001B[0;31m         return super().drop(\n\u001B[0m\u001B[1;32m   4309\u001B[0m             \u001B[0mlabels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4310\u001B[0m             \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36mdrop\u001B[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[1;32m   4151\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;32min\u001B[0m \u001B[0maxes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4152\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4153\u001B[0;31m                 \u001B[0mobj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_drop_axis\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4154\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4155\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0minplace\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m_drop_axis\u001B[0;34m(self, labels, axis, level, errors)\u001B[0m\n\u001B[1;32m   4186\u001B[0m                 \u001B[0mnew_axis\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4187\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4188\u001B[0;31m                 \u001B[0mnew_axis\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4189\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreindex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0maxis_name\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnew_axis\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4190\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mdrop\u001B[0;34m(self, labels, errors)\u001B[0m\n\u001B[1;32m   5589\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0many\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5590\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0merrors\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;34m\"ignore\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5591\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{labels[mask]} not found in axis\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   5592\u001B[0m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mindexer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m~\u001B[0m\u001B[0mmask\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5593\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdelete\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: \"['PRODUCT_GROUP' 'CUSTOMER_GROUP' 'ONLINE_PURCHASE'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# load in the data\n",
    "azdias = pd.read_csv('../data/azdias.csv')\n",
    "customers = pd.read_csv('../data/customers.csv')\n",
    "attributes = pd.read_excel('../data/attributes.xlsx', index_col = 1)\n",
    "\n",
    "#drop 3 of the customer columns which are missing in the global population dataset\n",
    "customers_clean = customers.drop(['PRODUCT_GROUP', 'CUSTOMER_GROUP', 'ONLINE_PURCHASE'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "XGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: image not found']\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mXGBoostError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-374368bd8bfb>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear_model\u001B[0m \u001B[0;32mimport\u001B[0m  \u001B[0mLogisticRegression\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcross_val_score\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mGridSearchCV\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mStratifiedKFold\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mxgboost\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mXGBClassifier\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     22\u001B[0m \u001B[0;31m# from bayes_opt import BayesianOptimization\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetrics\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mroc_curve\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mauc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDMatrix\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDeviceQuantileDMatrix\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBooster\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mtraining\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcv\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrabit\u001B[0m  \u001B[0;31m# noqa\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/core.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    193\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    194\u001B[0m \u001B[0;31m# load the XGBoost library globally\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 195\u001B[0;31m \u001B[0m_LIB\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_load_lib\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    196\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/core.py\u001B[0m in \u001B[0;36m_load_lib\u001B[0;34m()\u001B[0m\n\u001B[1;32m    176\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mlib_success\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    177\u001B[0m         \u001B[0mlibname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbasename\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlib_paths\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 178\u001B[0;31m         raise XGBoostError(\n\u001B[0m\u001B[1;32m    179\u001B[0m             \u001B[0;34m'XGBoost Library ({}) could not be loaded.\\n'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlibname\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    180\u001B[0m             \u001B[0;34m'Likely causes:\\n'\u001B[0m \u001B[0;34m+\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mXGBoostError\u001B[0m: XGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: image not found']\n"
     ]
    }
   ],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import plotly.express as px\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "# from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from lightgbm import LGBMClassifier\n",
    "from utils import  *\n",
    "# import skopt\n",
    "# from skopt import BayesSearchCV\n",
    "# class BayesSearchCV(BayesSearchCV):\n",
    "#     def _run_search(self, x): raise BaseException('Use newer skopt')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "azdias = pd.read_csv('data/azdias.csv')\n",
    "customers = pd.read_csv('data/customers.csv')\n",
    "attributes = pd.read_excel('data/attributes.xlsx', index_col = 1)\n",
    "\n",
    "#drop 3 of the customer columns which are missing in the global population dataset\n",
    "customers_clean = customers.drop(['PRODUCT_GROUP', 'CUSTOMER_GROUP', 'ONLINE_PURCHASE'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of the datasets\n",
    "print('azdias dimensions: ' + str(azdias.shape))\n",
    "print('customers dimensions: ' + str(customers.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#azdias first 5 rows\n",
    "azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desription of the azdias frame (Unnamed: 0 looks like repeats the index and LNR is the ID of each person)\n",
    "azdias.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for object values within the dataset\n",
    "print(cat_check(azdias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we can see the there are some X and XX values that should be corrected in the following columns\n",
    "print(azdias['CAMEO_DEU_2015'].unique())\n",
    "print(azdias['CAMEO_DEUG_2015'].unique())\n",
    "print(azdias['CAMEO_INTL_2015'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EINGEFUEGT_AM is a datetime column and OST_WEST_KZ should be mapped to binary values \n",
    "all transpormations are done in feature_transform() in file utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the attributes xls file to map the 0,-1,9 values to -1 \n",
    "azdias = unknown_unify(azdias, attributes)\n",
    "customers = unknown_unify(customers, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing vaues per feature histogram\n",
    "col_azdiaz = azdias.isnull().sum()/len(azdias)\n",
    "col_customers = customers.isnull().sum()/len(customers)\n",
    "plt.hist(col_azdiaz[col_azdiaz > 0.3], fc=(1, 0, 0, 0.5), label='Azdias')\n",
    "plt.hist(col_customers[col_customers > 0.3], fc=(0, 0, 1, 0.5), label='Customers')\n",
    "plt.xlabel('Percentage Missing Values')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram missing values per row\n",
    "bins = 50\n",
    "plt.hist(azdias.isnull().sum(axis=1), bins, fc=(1, 0, 0, 0.5), label='Azdias')\n",
    "plt.hist(customers.isnull().sum(axis=1), bins, fc=(0, 0, 1, 0.5), label='Customers')\n",
    "plt.xlabel('Number of missing values')\n",
    "plt.ylabel('Number of Rows')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display features with more than 30% missing values\n",
    "missing_values = azdias.isnull().sum()/len(azdias)\n",
    "azdias[missing_values[missing_values > 0.30].index].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####      ALTER_KIND features mark the age of children. Having a lot of NaN values is normal and dropping the features may result in loosing information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming some of the features and removing incorrect values\n",
    "azdias = feature_transform(azdias)\n",
    "customers = feature_transform(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of DECADE feature\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "sns.distplot(azdias['PRAEGENDE_JUGENDJAHRE_DECADE'], norm_hist = True, label='Azdias')\n",
    "sns.distplot(customers['PRAEGENDE_JUGENDJAHRE_DECADE'], norm_hist = True, label='Customers')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of INCOME feature\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "sns.distplot(azdias['HH_EINKOMMEN_SCORE'], norm_hist = True, label='Azdias')\n",
    "sns.distplot(customers['HH_EINKOMMEN_SCORE'], norm_hist = True, label='Customers')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of VACATION HABITS feature\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "sns.distplot(azdias['GFK_URLAUBERTYP'], norm_hist = True, label='Azdias')\n",
    "sns.distplot(customers['GFK_URLAUBERTYP'], norm_hist = True, label='Customers')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Unsupervised model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsupervised data preprocessing\n",
    "pipeline_unsup = Pipeline([  ('impute', SimpleImputer(strategy= 'constant', fill_value = -1)),\n",
    "                             ('scale', StandardScaler()), \n",
    "                             ('pca' , PCA()),\n",
    "                        ])\n",
    "\n",
    "#fit and transform the sets\n",
    "azdias_pca = pipeline_unsup.fit_transform(azdias)\n",
    "customers_pca =  pipeline_unsup.transform(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate PCA features to use in the clustering model\n",
    "fig = plt.figure(figsize = (14,8))\n",
    "plt.bar(list(range(0, len(pipeline_unsup[2].explained_variance_ratio_))), pipeline_unsup[2].explained_variance_ratio_)\n",
    "plt.xlabel('PCA components')\n",
    "plt.ylabel('Variance Ratio')\n",
    "plt.xlim(-1, 100)\n",
    "plt.show()\n",
    "\n",
    "pipeline_unsup[2].explained_variance_ratio_[:130].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#calculate number of clusters for the KMeans++ model:\n",
    "score = []\n",
    "\n",
    "for i in range(2,15):\n",
    "    clt = KMeans(n_clusters = i)\n",
    "    global_cluster = clt.fit(azdias_pca[:, :130])\n",
    "    score.append(global_cluster.inertia_)\n",
    "    print(score[i-2])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plow elbow curve\n",
    "fig = plt.figure(figsize = (12,8))\n",
    "sns.lineplot(x = list(range(2,15)), y = score)\n",
    "plt.xlabel('Cluster Number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init and fit the Kmeans++ with the desirable number of clusters (10)\n",
    "clt = KMeans(n_clusters = 8)\n",
    "\n",
    "azdiaz_cluster = clt.fit(azdias_pca[:, :130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict cluster labels for arvato customerz\n",
    "customers_cluster = clt.predict(customers_pca[:, :130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the distribution between azdias and customers data inside the clusters\n",
    "fig = plt.figure(figsize = (13,8))\n",
    "sns.distplot(azdiaz_cluster.labels_)\n",
    "sns.distplot(customers_cluster)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a look at the features between the two most different clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify important feature differences between the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_clst = azdias\n",
    "azdias_clst['Cluster'] = azdiaz_cluster.labels_\n",
    "\n",
    "customers_clst = customers\n",
    "customers_clst['Cluster'] = customers_cluster\n",
    "\n",
    "azdias_clst = azdias_clst[azdias_clst['Cluster'] == 4]\n",
    "customers_clst = customers_clst[customers_clst['Cluster'] == 0]\n",
    "\n",
    "#get mean difference between features\n",
    "diff = pd.DataFrame({'Cluster_4': azdias_clst.mean(), 'Cluster_0': customers_clst.mean()})\n",
    "diff['delta'] = abs(diff['Cluster_4'] - diff['Cluster_0'])\n",
    "diff.head(20).sort_values(['delta'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CAMEO_DEUG_2015: Customers are more likely to be 'established middle class' against Cluster 3 values : low-consumption middleclass.\n",
    "\n",
    "##### CAMEO_INTL_2015: Prosperous households are more likely to be customers.\n",
    "\n",
    "##### ANZ_PERSONEN: Households with more adult people use the company's products.\n",
    "\n",
    "##### ALTER_HH: The main age within the households of customers is lower than the underrepresented cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean the train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('data/mail_train.csv')\n",
    "mailout_test = pd.read_csv('data/mail_test.csv')\n",
    "\n",
    "\n",
    "#most of the features has less than 30% missing values similar to azdias and customers datasets\n",
    "sns.distplot(mailout_train.isnull().sum()/len(mailout_train))\n",
    "sns.distplot(mailout_test.isnull().sum()/len(mailout_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process load data\n",
    "attributes = pd.read_excel('data/attributes.xlsx', index_col = 1)\n",
    "\n",
    "#train data\n",
    "mailout_train = unknown_unify(mailout_train, attributes)\n",
    "X = feature_transform(mailout_train)\n",
    "\n",
    "#test data\n",
    "mailout_test = unknown_unify(mailout_test, attributes)\n",
    "X_sub = feature_transform(mailout_test)\n",
    "\n",
    "#----!\n",
    "\n",
    "y = X['RESPONSE']\n",
    "X.drop(['RESPONSE'], axis = 1, inplace = True)\n",
    "\n",
    "print(X.shape, X_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = y.value_counts().index, y = y.value_counts())\n",
    "plt.xlabel('RESPONSE')\n",
    "plt.ylabel('Count')\n",
    "print(y.value_counts()/y.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test the initial models for overall performance based on the competition metri ROC_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of models with default hyerparameters using models with capability for probabilistic prediction\n",
    "model_list = {'LR':LogisticRegression(), 'RF' : RandomForestClassifier(),'LGBM':  LGBMClassifier(), 'XGB':  XGBClassifier()}\n",
    "\n",
    "#loop through the models and evaluate basic performance\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "for keys in model_list:\n",
    "    #plot series\n",
    "    pipeline = Pipeline([('impute', SimpleImputer(strategy= 'constant', fill_value = -1)),('scale', StandardScaler()),  ('clf', model_list[keys])])\n",
    "    plot_roc(pipeline,X, y, keys)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "It's clear that with the default parameters LGBMClassifier gives the highest results. After some parameter tuning and submission to the competition actually XGB gave higher scores so I will continue with BayesianOptimization of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('impute', SimpleImputer(strategy= 'constant', fill_value = -1)), \n",
    "                     ('clf', XGBClassifier())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Optimization of XGB hyperparameters. \n",
    "\n",
    "Using 5 fold StratifiedKFold to evaluate each step. Here I use only 10 steps for example and try to keep the notebook as clean as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 10\n",
    "SEED = 42\n",
    "\n",
    "bayes_cv_tuner_xg = BayesSearchCV(\n",
    "    estimator = pipeline,\n",
    "    search_spaces = {\n",
    "        'clf__learning_rate': (0.001, 0.9, 'log-uniform'),\n",
    "        'clf__max_depth': (2, 10),\n",
    "        'clf__min_child_weight': (1, 10),\n",
    "        'clf__gamma': (0.0, 1.0, 'uniform'),\n",
    "        'clf__subsample': (0.5, 1.0, 'uniform'),\n",
    "        'clf__colsample_bytree': (0.5, 1.0, 'uniform'),\n",
    "        'clf__reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
    "        'clf__n_estimators': (50, 500),\n",
    "        'clf__scale_pos_weight': (1,90)\n",
    "        \n",
    "    },    \n",
    "    scoring = 'roc_auc',\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=5,\n",
    "        shuffle=True,\n",
    "        random_state= SEED\n",
    "    ),\n",
    "    n_jobs = -1,\n",
    "    n_iter = ITERATIONS,   \n",
    "    verbose = 0,\n",
    "    refit = True,\n",
    "    random_state = np.random.RandomState(50)\n",
    ")\n",
    "\n",
    "def status_print(optim_result):\n",
    "    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n",
    "    \n",
    "    # Get all the models tested so far in DataFrame format },\n",
    "    all_models = pd.DataFrame(bayes_cv_tuner_xg.cv_results_)    \n",
    "    \n",
    "    # Get current parameters and the best parameters    \n",
    "    best_params = pd.Series(bayes_cv_tuner_xg.best_params_)\n",
    "    \n",
    "    print('Model #{}\\nBest ROC-AUC: {}\\nBest params: {}\\n'.format(\n",
    "        len(all_models),\n",
    "        np.round(bayes_cv_tuner_xg.best_score_, 4),\n",
    "        bayes_cv_tuner_xg.best_params_\n",
    "    ))    \n",
    "    \n",
    "    # Save all model results\n",
    "    clf_name = bayes_cv_tuner_xg.estimator.__class__.__name__\n",
    "    all_models.to_csv(clf_name+\"_cv_results.csv\")\n",
    "    \n",
    "%time\n",
    "result_xg = bayes_cv_tuner_xg.fit(X, y, callback=status_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized algorithm\n",
    "parameters= dict([('colsample_bytree', 0.6888348010130712), ('gamma', 0.43702345115978325), ('learning_rate', 0.0030871248366675184), ('max_depth', 3), ('min_child_weight', 5), ('n_estimators', 376), ('reg_alpha', 0.9682055105813826), ('scale_pos_weight', 86), ('subsample', 0.9162583886152913)])\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('impute', SimpleImputer(strategy= 'constant', fill_value = -1)), \n",
    "                     ('clf', XGBClassifier(**parameters))\n",
    "                    ])\n",
    "\n",
    "fig\n",
    "fig1 = plt.figure(figsize = (10,10))\n",
    "plot_roc(pipeline,X, y, 'XGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition (4th position in the leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At the end highest result was achieved after using the model with the first 200 features based on the feature importances of the XGB. Maybe it's better to keep the slightly lower score based on all features because the test at Kaggle is done only at 30% of the test data. Using all the features should generalize better and probably will give better results on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 200\n",
    "\n",
    "#get the feature importances DataFrame and plot the barchart\n",
    "model_fi = XGBClassifier( verbose = False).fit( X, y)\n",
    "fi = pd.DataFrame({'features':X.columns, 'importance' : model_fi.feature_importances_/ model_fi.feature_importances_.sum()}).sort_values(['importance'], ascending = False)\n",
    "fig = px.bar(x = fi['features'], y = fi['importance'])\n",
    "fig.show()\n",
    "\n",
    "#reduce the ftrain and test data based on the important features\n",
    "X_reduced = X[fi['features'].head(num_features).to_list()]\n",
    "X_sub_reduced = X_sub[fi['features'].head(num_features).to_list()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the dictionaty with optimized parameters result of the BayesianOptimization\n",
    "model = XGBClassifier(**parameters)\n",
    "\n",
    "#predefined funciton for trainings and exporting the final model\n",
    "predict_sub(X_reduced, y, model, X_sub_reduced)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}