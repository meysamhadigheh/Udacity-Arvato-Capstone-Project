{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importing libraries:\n",
    "\n",
    "from .utils import *\n",
    "import joblib\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as Pipeline_imb\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve, train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "azdias = pd.read_csv('../data/azdias.csv', sep=';')\n",
    "customers=pd.read_csv('../data/customers.csv',sep=';')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md load needed files\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating a list of variables that overcomes the threshold for nan values:\n",
    "nan_threshold = 0.35 # 35%\n",
    "\n",
    "# Nan proportion, using general population dataframe (more observations):\n",
    "var_nan_prop = azdias.isnull().mean()\n",
    "\n",
    "# List:\n",
    "nan_list = list()\n",
    "for i in range(len(var_nan_prop)):\n",
    "    if var_nan_prop[i] >= nan_threshold:\n",
    "        nan_list.append(var_nan_prop.index.values[i])\n",
    "\n",
    "print('{} columns with more than {}% of nan values.'.format(len(nan_list), nan_threshold*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating function that delete the columns listed in nan_list:\n",
    "def eliminate_nan_columns(df, nan_cols = nan_list):\n",
    "    '''\n",
    "    It deletes dataframe columns in nan_cols list.\n",
    "\n",
    "    Inputs:\n",
    "    df: original dataframe;\n",
    "    nan_cols:list of columns to be deleted.\n",
    "\n",
    "    Output:\n",
    "    df: dataframe updated without nan_cols.\n",
    "    '''\n",
    "    # Deleting nan_cols:\n",
    "    df.drop(columns = nan_cols, inplace = True)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2: Supervised Learning Model<a name=\"part2\"></a>\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load in the data:\n",
    "mailout_train = pd.read_csv('../data/mail_train.csv', sep=';')\n",
    "\n",
    "# Loading train data:\n",
    "#mailout_train = joblib.load('train')\n",
    "\n",
    "mailout_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Verifying class balance:\n",
    "mailout_train.RESPONSE.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Data Transformation<a name=\"datatrans\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the supervised learning task, the strategy will be less conservative, eliminating as few features as possible. Because of that, the process of selecting columns will be simplified, increasing the threshold percentage for *nan* values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Joining nan and unknown values:\n",
    "mailout_train = join_nan_with_unknown(mailout_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating a list of variables that overcomes the threshold for nan values:\n",
    "new_nan_threshold = 0.5 # 50%\n",
    "\n",
    "# Nan proportion, using general population dataframe (more observations):\n",
    "var_nan_prop = mailout_train.isnull().mean()\n",
    "\n",
    "# List:\n",
    "new_nan_list = list()\n",
    "for i in range(len(var_nan_prop)):\n",
    "    if var_nan_prop[i] >= new_nan_threshold:\n",
    "        new_nan_list.append(var_nan_prop.index.values[i])\n",
    "\n",
    "print('{} columns with more than {}% of nan values.'.format(len(new_nan_list), new_nan_threshold*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Updating dtypes_dict:\n",
    "new_dtypes_dict = dtypes_dict.copy()\n",
    "new_dtypes_dict['AGER_TYP'] = 'cat'\n",
    "new_dtypes_dict['D19_GESAMT_ANZ_24'] = 'num'\n",
    "new_dtypes_dict['D19_GESAMT_DATUM'] = 'num'\n",
    "new_dtypes_dict['D19_GESAMT_OFFLINE_DATUM'] = 'num'\n",
    "new_dtypes_dict['D19_GESAMT_ONLINE_DATUM'] = 'num'\n",
    "new_dtypes_dict['D19_KONSUMTYP'] = 'cat'\n",
    "new_dtypes_dict['D19_KONSUMTYP_MAX'] = 'num'\n",
    "new_dtypes_dict['D19_SONSTIGE'] = 'num'\n",
    "new_dtypes_dict['D19_SOZIALES'] = 'num'\n",
    "new_dtypes_dict['D19_VERSAND_DATUM'] = 'num'\n",
    "new_dtypes_dict['D19_VERSAND_OFFLINE_DATUM'] = 'num'\n",
    "new_dtypes_dict['D19_VOLLSORTIMENT'] = 'num'\n",
    "new_dtypes_dict['EXTSEL992'] = 'num'\n",
    "new_dtypes_dict['GEBURTSJAHR'] = 'num'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Target column:\n",
    "y = mailout_train.RESPONSE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def supervised_data_transformation(df, test_set):\n",
    "    '''\n",
    "    It applies all the data transformation steps before treating nan values.\n",
    "\n",
    "    Input:\n",
    "    df: original dataframe;\n",
    "    train_set: boolean indicating whether it's train or test set.\n",
    "\n",
    "    Output:\n",
    "    df: transformed dataframe.\n",
    "    '''\n",
    "    if test_set:\n",
    "        # Joining nan and unknown values:\n",
    "        df = join_nan_with_unknown(df)\n",
    "\n",
    "        # Columns do drop:\n",
    "        drop_cols = ['LNR', 'EINGEFUEGT_AM', 'EINGEZOGENAM_HH_JAHR']\n",
    "\n",
    "    else:\n",
    "        drop_cols = ['LNR', 'EINGEFUEGT_AM', 'EINGEZOGENAM_HH_JAHR', 'RESPONSE']\n",
    "\n",
    "    # Eliminating nan columns:\n",
    "    df = eliminate_nan_columns(df, nan_cols = new_nan_list)\n",
    "\n",
    "    # Dropping useless columns:\n",
    "    df = df.drop(columns = drop_cols)\n",
    "\n",
    "    # Changing column dtypes:\n",
    "    df = change_dtypes(df, dtypes = new_dtypes_dict)\n",
    "\n",
    "    # Feature engineering I:\n",
    "    df = feature_engineer(df)\n",
    "\n",
    "    # Feature engineering II:\n",
    "    df = transform_cameo_deu(df)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Applying data transformations on train set:\n",
    "mailout_train = supervised_data_transformation(mailout_train, test_set = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next step, different pipelines will be built in order to treat *nan* values differently, according to column dtype."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating dtypes list:\n",
    "numerical_cols_list = list()\n",
    "binary_cols_list = list()\n",
    "categorical_cols_list = list()\n",
    "\n",
    "# Appending columns to lists:\n",
    "for col in mailout_train.columns:\n",
    "    try:\n",
    "        dtype = new_dtypes_dict[col]\n",
    "    except:\n",
    "        dtype = new_feat_dtypes_dict[col]\n",
    "\n",
    "    if dtype == 'num':\n",
    "        numerical_cols_list.append(col)\n",
    "    elif dtype == 'cat':\n",
    "        categorical_cols_list.append(col)\n",
    "    else:\n",
    "        binary_cols_list.append(col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pipeline for treating nan values:\n",
    "# Numerical features: nan values will be imputed using the 'median', and then StandardScaler will be applied:\n",
    "num_features = numerical_cols_list\n",
    "num_transformer = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical features will be one-hot-encoded:\n",
    "cat_features = categorical_cols_list\n",
    "cat_transformer = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('cat_ohe', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "# Binary features with nan values will also be one-hot-encoded:\n",
    "bin_features = list(mailout_train[binary_cols_list].columns[mailout_train[binary_cols_list].isnull().sum() > 0])\n",
    "bin_transformer = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('bin_ohe', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "# Encapsulating transformations:\n",
    "preproc = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('num', num_transformer, num_features),\n",
    "        ('cat', cat_transformer, cat_features),\n",
    "        ('bin', bin_transformer, bin_features)\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Analyzing Learning Curves<a name=\"learningcurve\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To better understand which algorithms would be a better choice, the learning curves related to a few algorithms are going to be drawn:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining a function to plot learning curves:\n",
    "def plot_learning_curves(X, y, model, steps):\n",
    "    '''\n",
    "    It plots the learning curve for the desired algorithm.\n",
    "\n",
    "    Input:\n",
    "    X: predictive features;\n",
    "    y: target feature;\n",
    "    model: instantiated object of the algorithm to be trained;\n",
    "    steps: integer defining the steps for training size.\n",
    "    '''\n",
    "    # Setting train_sizes:\n",
    "    train_sizes = np.linspace(0.1, 1.0, steps)\n",
    "\n",
    "    # Applying sklearn learning_curve:\n",
    "    train_size, train_score, test_score = learning_curve(model, X, y,\n",
    "                                                         scoring = 'roc_auc',\n",
    "                                                         train_sizes = train_sizes)\n",
    "    # Computing average train and test scores:\n",
    "    avg_train_score = np.mean(train_score, axis = 1)\n",
    "    avg_test_score = np.mean(test_score, axis = 1)\n",
    "\n",
    "    # Printing results:\n",
    "    print(\"ROC_AUC train score: {:.2f}\".format(avg_train_score[-1]))\n",
    "    print(\"ROC_AUC valid. score: {:.2f}\".format(avg_test_score[-1]))\n",
    "\n",
    "    # Creating learning curve plot:\n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (15, 5))\n",
    "\n",
    "    sns.lineplot(x = train_sizes * 100, y = avg_train_score,\n",
    "                 marker = 'o', color = 'tomato',\n",
    "                 label = 'Train', ax = ax).set(xlabel = 'Training Set Percentage',\n",
    "                                               ylabel = 'Model Score',\n",
    "                                               title = 'Learning Curve')\n",
    "\n",
    "    sns.lineplot(x = train_sizes * 100, y = avg_test_score,\n",
    "                 marker = 'o', color = 'springgreen',\n",
    "                 label = 'Validation', ax = ax)\n",
    "\n",
    "    sns.despine(left=True, top = True)\n",
    "\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# XGBClassifier\n",
    "\n",
    "# Print classifier's name:\n",
    "print('XGBClassifier')\n",
    "\n",
    "# Defining machine learning pipeline:\n",
    "ml_pipe = Pipeline(steps = [\n",
    "    ('preprocessing', preproc),\n",
    "    ('clf', XGBClassifier())\n",
    "])\n",
    "\n",
    "# Creating a randomized version of the train dataframe:\n",
    "df = mailout_train.copy()\n",
    "df['RESPONSE'] = list(y.values)\n",
    "rand_df = df.sample(frac = 1)\n",
    "\n",
    "# Creating X_train and y_train objects:\n",
    "rand_y = rand_df['RESPONSE']\n",
    "rand_X = rand_df.drop(['RESPONSE'], axis = 1)\n",
    "\n",
    "# Plotting learning curve:\n",
    "plot_learning_curves(rand_X, rand_y, ml_pipe, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clearly, the learning curves are not converging: the average score on the training set stands in high values, while validation scores are poor.\n",
    "\n",
    "It means that the **XGBClassifier** is overfitting, and the model is not actually learning or generalizing, explaining the low scores in the validation set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GradientBoosting\n",
    "\n",
    "# Print classifier's name:\n",
    "print('GradientBoostingClassifier')\n",
    "\n",
    "# Defining machine learning pipeline:\n",
    "ml_pipe = Pipeline(steps = [\n",
    "    ('preprocessing', preproc),\n",
    "    ('clf', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Creating a randomized version of the train dataframe:\n",
    "df = mailout_train.copy()\n",
    "df['RESPONSE'] = list(y.values)\n",
    "rand_df = df.sample(frac = 1)\n",
    "\n",
    "# Creating X_train and y_train objects:\n",
    "rand_y = rand_df['RESPONSE']\n",
    "rand_X = rand_df.drop(['RESPONSE'], axis = 1)\n",
    "\n",
    "# Plotting learning curve:\n",
    "plot_learning_curves(rand_X, rand_y, ml_pipe, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the other hand, **GradientBoostingClassifier** represents a better option once the learning curves are converging, and the validation score is consistently improving while the algorithm receives more information."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# AdaBoostClassifier\n",
    "\n",
    "# Print classifier's name:\n",
    "print('AdaBoostClassifier')\n",
    "\n",
    "# Defining machine learning pipeline:\n",
    "ml_pipe = Pipeline(steps = [\n",
    "    ('preprocessing', preproc),\n",
    "    ('clf', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "# Creating a randomized version of the train dataframe:\n",
    "df = mailout_train.copy()\n",
    "df['RESPONSE'] = list(y.values)\n",
    "rand_df = df.sample(frac = 1)\n",
    "\n",
    "# Creating X_train and y_train objects:\n",
    "rand_y = rand_df['RESPONSE']\n",
    "rand_X = rand_df.drop(['RESPONSE'], axis = 1)\n",
    "\n",
    "# Plotting learning curve:\n",
    "plot_learning_curves(rand_X, rand_y, ml_pipe, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**AdaBoostClassifier** shows a similar pattern when comparing to the GradientBoostingClassifier. However, its validation score was not as good.\n",
    "\n",
    "Considering the models that didn't overfit, **GradientBoostingClassifier** seems a better option:\n",
    "* learning curves seem to keep converging, showing perspective of improvements;\n",
    "* validation score achieved higher values, indicating that it performs better on unseen data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Training Classifier<a name=\"training\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that the learning curve was observed for different algorithms, and the *GradientBoostingClassifier* was chosen as a better option, a few steps will be followed:\n",
    "* defining data pipeline;\n",
    "* setting different parameters for model tuning;\n",
    "* *GridSearchCV* to optimize parameters' combination.\n",
    "\n",
    "Since the data is highly unbalanced, the evaluation metric will be the `roc_auc` score."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.1 Training on Unbalanced Data<a name=\"t1\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this first attempt, the unbalance seen in the classes will not be treated."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining machine learning pipeline:\n",
    "gbc_ml_pipe = Pipeline(steps = [\n",
    "    ('preprocessing', preproc),\n",
    "    ('clf', GradientBoostingClassifier(learning_rate = 0.1,\n",
    "                                       n_estimators = 150,\n",
    "                                       random_state = 301))\n",
    "])\n",
    "\n",
    "# Setting parameters to be tested:\n",
    "params = {'clf__min_samples_split': [2, 4],\n",
    "          'clf__max_depth': [3, 5],\n",
    "          'clf__max_features': [None, 'auto']\n",
    "}\n",
    "\n",
    "# Grid search + ML pipleine:\n",
    "gbc_clf = GridSearchCV(gbc_ml_pipe, param_grid = params, scoring = 'roc_auc', verbose = 2)\n",
    "\n",
    "# Training model:\n",
    "gbc_clf.fit(mailout_train, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking best parameters:\n",
    "print(gbc_clf.best_params_)\n",
    "\n",
    "# Checking best score:\n",
    "print('Best ROC_AUC score: {:.2f}'.format(gbc_clf.best_score_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving model:\n",
    "filename = 'gbc_model.pkl'\n",
    "pickle.dump(gbc_clf, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.2 Training on Balanced Data<a name=\"t2\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this second attempt, the `SMOTE` technique will be included in the machine learning pipeline.\n",
    "\n",
    "The purpose is to see if the `roc_auc` score increases, once the unbalance is treated."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining machine learning pipeline:\n",
    "gbc_smote_ml_pipe = Pipeline_imb(steps = [\n",
    "    ('preprocessing', preproc),\n",
    "    ('smote', SMOTE(random_state = 501)),\n",
    "    ('clf', GradientBoostingClassifier(learning_rate = 0.1,\n",
    "                                       n_estimators = 150,\n",
    "                                       random_state = 501))\n",
    "])\n",
    "\n",
    "# Setting parameters to be tested:\n",
    "params_smote = {'smote__sampling_strategy': [0.3, 0.5, 1.0],\n",
    "                'clf__min_samples_split': [2, 4],\n",
    "                'clf__max_depth': [3],\n",
    "                'clf__max_features': [None]\n",
    "}\n",
    "\n",
    "# Grid search + ML pipleine:\n",
    "gbc_smote_clf = GridSearchCV(gbc_smote_ml_pipe, param_grid = params_smote, scoring = 'roc_auc', verbose = 2)\n",
    "\n",
    "# Training model:\n",
    "gbc_smote_clf.fit(mailout_train, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking best parameters:\n",
    "print(gbc_smote_clf.best_params_)\n",
    "\n",
    "# Checking best score:\n",
    "print('Best ROC_AUC score: {:.2f}'.format(gbc_smote_clf.best_score_))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving model:\n",
    "filename = 'gbc_smote_clf.pkl'\n",
    "pickle.dump(gbc_clf, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This strategy to deal with the class unbalance didn't result in a better score. Because of that, in the next attempts, unbalance will not be treated."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.3 Using Information Level and PCA Transformation<a name=\"t3\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this third attempt, a similar approach used during the cluster analysis will be performed here. In this case, data will be treated differently not only considering the columns' dtypes, but also the information level related to the columns.\n",
    "\n",
    "As an example, `person` information level will be split into:\n",
    "* numerical features;\n",
    "* categorical features;\n",
    "* binary features.\n",
    "\n",
    "In the most generic levels of information like `microcell` and `macrocell`, dimensionality reduction will be applied (`PCA` for numerical features, and `TruncatedSVD` for sparse matrix (categorical columns after the one-hot encoding process).\n",
    "\n",
    "This way, when applied, the dimensionality reduction will result in components representing one single level of information."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Updating info_level dictionary:\n",
    "new_info_level = info_level.copy()\n",
    "\n",
    "# Adding columns:\n",
    "# Person:\n",
    "new_info_level['person'].append('AGER_TYP')\n",
    "new_info_level['person'].append('GEBURTSJAHR')\n",
    "\n",
    "# Household:\n",
    "new_info_level['household'].append('D19_GESAMT_ANZ_24')\n",
    "new_info_level['household'].append('D19_GESAMT_DATUM')\n",
    "new_info_level['household'].append('D19_GESAMT_OFFLINE_DATUM')\n",
    "new_info_level['household'].append('D19_GESAMT_ONLINE_DATUM')\n",
    "new_info_level['household'].append('D19_KONSUMTYP')\n",
    "new_info_level['household'].append('D19_KONSUMTYP_MAX')\n",
    "new_info_level['household'].append('D19_SONSTIGE')\n",
    "new_info_level['household'].append('D19_SOZIALES')\n",
    "new_info_level['household'].append('D19_VERSAND_DATUM')\n",
    "new_info_level['household'].append('D19_VERSAND_OFFLINE_DATUM')\n",
    "new_info_level['household'].append('D19_VOLLSORTIMENT')\n",
    "\n",
    "# Macrocell:\n",
    "new_info_level['macrocell'].append('EXTSEL992')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dividing Person features into numerical, categorical and binary:\n",
    "pers_num_features = list()\n",
    "\n",
    "pers_cat_features = list()\n",
    "\n",
    "pers_bin_features = list()\n",
    "\n",
    "# Adding columns to lists:\n",
    "for pers_col in new_info_level['person']:\n",
    "    try:\n",
    "        dtype = new_dtypes_dict[pers_col]\n",
    "    except:\n",
    "        dtype = new_feat_dtypes_dict[pers_col]\n",
    "\n",
    "    if dtype == 'num':\n",
    "        pers_num_features.append(pers_col)\n",
    "    elif dtype == 'cat':\n",
    "        pers_cat_features.append(pers_col)\n",
    "    else:\n",
    "        pers_bin_features.append(pers_col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dividing Household features into numerical, categorical and binary:\n",
    "hh_num_features = list()\n",
    "\n",
    "hh_cat_features = list()\n",
    "\n",
    "hh_bin_features = list()\n",
    "\n",
    "# Adding columns to lists:\n",
    "for hh_col in new_info_level['household']:\n",
    "    try:\n",
    "        dtype = new_dtypes_dict[hh_col]\n",
    "    except:\n",
    "        dtype = new_feat_dtypes_dict[hh_col]\n",
    "\n",
    "    if dtype == 'num':\n",
    "        hh_num_features.append(hh_col)\n",
    "    elif dtype == 'cat':\n",
    "        hh_cat_features.append(hh_col)\n",
    "    else:\n",
    "        hh_bin_features.append(hh_col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dividing Microcell features into numerical, categorical and binary:\n",
    "mic_num_features = list()\n",
    "\n",
    "mic_cat_features = list()\n",
    "\n",
    "mic_bin_features = list()\n",
    "\n",
    "# Adding columns to lists:\n",
    "for mic_col in new_info_level['microcell']:\n",
    "    try:\n",
    "        dtype = new_dtypes_dict[mic_col]\n",
    "    except:\n",
    "        dtype = new_feat_dtypes_dict[mic_col]\n",
    "\n",
    "    if dtype == 'num':\n",
    "        mic_num_features.append(mic_col)\n",
    "    elif dtype == 'cat':\n",
    "        mic_cat_features.append(mic_col)\n",
    "    else:\n",
    "        mic_bin_features.append(mic_col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dividing Macrocell features into numerical, categorical and binary:\n",
    "mac_num_features = list()\n",
    "\n",
    "mac_cat_features = list()\n",
    "\n",
    "mac_bin_features = list()\n",
    "\n",
    "# Adding columns to lists:\n",
    "for mac_col in new_info_level['macrocell']:\n",
    "    try:\n",
    "        dtype = new_dtypes_dict[mac_col]\n",
    "    except:\n",
    "        dtype = new_feat_dtypes_dict[mac_col]\n",
    "\n",
    "    if dtype == 'num':\n",
    "        mac_num_features.append(mac_col)\n",
    "    elif dtype == 'cat':\n",
    "        mac_cat_features.append(mac_col)\n",
    "    else:\n",
    "        mac_bin_features.append(mac_col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dividing Community features into numerical, categorical and binary:\n",
    "com_num_features = list()\n",
    "\n",
    "com_cat_features = list()\n",
    "\n",
    "com_bin_features = list()\n",
    "\n",
    "# Adding columns to lists:\n",
    "for com_col in new_info_level['community']:\n",
    "    try:\n",
    "        dtype = new_dtypes_dict[com_col]\n",
    "    except:\n",
    "        dtype = new_feat_dtypes_dict[com_col]\n",
    "\n",
    "    if dtype == 'num':\n",
    "        com_num_features.append(com_col)\n",
    "    elif dtype == 'cat':\n",
    "        com_cat_features.append(com_col)\n",
    "    else:\n",
    "        com_bin_features.append(com_col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pipeline for treating nan values and applying PCA to different information levels:\n",
    "\n",
    "# PERSON level:\n",
    "# Numerical features: nan values will be imputed using the 'median', and then StandardScaler will be applied:\n",
    "# pers_num_features\n",
    "pers_num_transformer = Pipeline(steps = [\n",
    "    ('pers_num_imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('pers_num_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical features will be one-hot-encoded:\n",
    "# pers_cat_features\n",
    "pers_cat_transformer = Pipeline(steps = [\n",
    "    ('pers_cat_imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('pers_cat_ohe', OneHotEncoder(handle_unknown = 'ignore')),\n",
    "    ('pers_cat_pca', TruncatedSVD(n_components = 9, random_state = 701))\n",
    "])\n",
    "\n",
    "# Binary features with nan values will also be one-hot-encoded:\n",
    "# pers_bin_features\n",
    "pers_bin_transformer = Pipeline(steps = [\n",
    "    ('pers_bin_imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('pers_bin_ohe', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# HOUSEHOLD level:\n",
    "# Numerical features: nan values will be imputed using the 'median', and then StandardScaler will be applied:\n",
    "# hh_num_features\n",
    "hh_num_transformer = Pipeline(steps = [\n",
    "    ('hh_num_imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('hh_num_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical features will be one-hot-encoded:\n",
    "# hh_cat_features\n",
    "hh_cat_transformer = Pipeline(steps = [\n",
    "    ('hh_cat_imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('hh_cat_ohe', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MICROCELL level:\n",
    "# Numerical features: nan values will be imputed using the 'median', and then StandardScaler will be applied:\n",
    "# mic_num_features\n",
    "mic_num_transformer = Pipeline(steps = [\n",
    "    ('mic_num_imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('mic_num_scaler', StandardScaler()),\n",
    "    ('mic_num_pca', PCA(n_components = 6, random_state = 702))\n",
    "])\n",
    "\n",
    "# Categorical features will be one-hot-encoded:\n",
    "# mic_cat_features\n",
    "mic_cat_transformer = Pipeline(steps = [\n",
    "    ('mic_cat_imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('mic_cat_ohe', OneHotEncoder(handle_unknown = 'ignore')),\n",
    "    ('mic_cat_pca', TruncatedSVD(n_components = 2, random_state = 703))\n",
    "])\n",
    "\n",
    "# Binary features with nan values will also be one-hot-encoded:\n",
    "# mic_bin_features\n",
    "mic_bin_transformer = Pipeline(steps = [\n",
    "    ('mic_bin_imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('mic_bin_ohe', OneHotEncoder(handle_unknown = 'ignore')),\n",
    "    ('mic_bin_pca', TruncatedSVD(n_components = 2, random_state = 704))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MACROCELL level:\n",
    "# Numerical features: nan values will be imputed using the 'median', and then StandardScaler will be applied:\n",
    "# mac_num_features\n",
    "mac_num_transformer = Pipeline(steps = [\n",
    "    ('mac_num_imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('mac_num_scaler', StandardScaler()),\n",
    "    ('mac_num_pca', PCA(n_components = 8, random_state = 705))\n",
    "])\n",
    "\n",
    "# Categorical features will be one-hot-encoded:\n",
    "# mac_cat_features\n",
    "mac_cat_transformer = Pipeline(steps = [\n",
    "    ('mac_cat_imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('mac_cat_ohe', OneHotEncoder(handle_unknown = 'ignore')),\n",
    "    ('mac_cat_pca', TruncatedSVD(n_components = 2, random_state = 706))\n",
    "])\n",
    "\n",
    "# Binary features with nan values will also be one-hot-encoded:\n",
    "# mac_bin_features\n",
    "mac_bin_transformer = Pipeline(steps = [\n",
    "    ('mac_bin_imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('mac_bin_ohe', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# COMMUNITY level:\n",
    "# Numerical features: nan values will be imputed using the 'median', and then StandardScaler will be applied:\n",
    "# com_num_features\n",
    "com_num_transformer = Pipeline(steps = [\n",
    "    ('com_num_imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('com_num_scaler', StandardScaler())\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Encapsulating transformations:\n",
    "preproc_pca = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('pers_num', pers_num_transformer, pers_num_features),\n",
    "        ('pers_cat', pers_cat_transformer, pers_cat_features),\n",
    "        ('pers_bin', pers_bin_transformer, pers_bin_features),\n",
    "        ('hh_num', hh_num_transformer, hh_num_features),\n",
    "        ('hh_cat', hh_cat_transformer, hh_cat_features),\n",
    "        ('mic_num', mic_num_transformer, mic_num_features),\n",
    "        ('mic_cat', mic_cat_transformer, mic_cat_features),\n",
    "        ('mic_bin', mic_bin_transformer, mic_bin_features),\n",
    "        ('mac_num', mac_num_transformer, mac_num_features),\n",
    "        ('mac_cat', mac_cat_transformer, mac_cat_features),\n",
    "        ('mac_bin', mac_bin_transformer, mac_bin_features),\n",
    "        ('com_num', com_num_transformer, com_num_features)\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Concatenating selected columns:\n",
    "selected_columns = pers_num_features + pers_cat_features + pers_bin_features + hh_num_features + hh_cat_features + mic_num_features + mic_cat_features + mic_bin_features + mac_num_features + mac_cat_features + mac_bin_features + com_num_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining machine learning pipeline:\n",
    "gbc_pca_ml_pipe = Pipeline(steps = [\n",
    "    ('preprocessing', preproc_pca),\n",
    "    ('clf', GradientBoostingClassifier(learning_rate = 0.1,\n",
    "                                       n_estimators = 150,\n",
    "                                       random_state = 701))\n",
    "])\n",
    "\n",
    "# Setting parameters to be tested:\n",
    "params_pca = {'clf__min_samples_split': [2, 4],\n",
    "              'clf__max_depth': [3, 5],\n",
    "              'clf__max_features': [None]\n",
    "}\n",
    "\n",
    "# Grid search + ML pipleine:\n",
    "gbc_pca_clf = GridSearchCV(gbc_pca_ml_pipe, param_grid = params_pca, scoring = 'roc_auc', verbose = 2)\n",
    "\n",
    "# Training model:\n",
    "gbc_pca_clf.fit(mailout_train[selected_columns], y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking best parameters:\n",
    "print(gbc_pca_clf.best_params_)\n",
    "\n",
    "# Checking best score:\n",
    "print('Best ROC_AUC score: {:.2f}'.format(gbc_pca_clf.best_score_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving model:\n",
    "filename = 'gbc_pca_clf.pkl'\n",
    "pickle.dump(gbc_pca_clf, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This approach resulted in a better score when comparing to the second approach, but still not as good as the first strategy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.4 Using PCA Transformation<a name=\"t4\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The fourth approach is a variation of the first one, but this time the dimensionality reduction will be applied to the data. Different from the third approach, information level will not be considered.\n",
    "\n",
    "This way, the `PCA` algorithm will be applied along in the machine learning pipeline, and the components will represent the whole data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining machine learning pipeline:\n",
    "gbc_pca2_pipe = Pipeline(steps = [\n",
    "    ('preprocessing', preproc),\n",
    "    ('pca', PCA(random_state = 901)),\n",
    "    ('clf', GradientBoostingClassifier(learning_rate = 0.1,\n",
    "                                       n_estimators = 150,\n",
    "                                       random_state = 301))\n",
    "])\n",
    "\n",
    "# Setting parameters to be tested:\n",
    "params_pca2 = {'pca__n_components': [70, 100, 150],\n",
    "               'clf__min_samples_split': [2, 4],\n",
    "               'clf__max_depth': [3],\n",
    "               'clf__max_features': [None]\n",
    "}\n",
    "\n",
    "# Grid search + ML pipleine:\n",
    "gbc_pca2_clf = GridSearchCV(gbc_pca2_pipe, param_grid = params_pca2, scoring = 'roc_auc', verbose = 2)\n",
    "\n",
    "# Training model:\n",
    "gbc_pca2_clf.fit(mailout_train, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking best parameters:\n",
    "print(gbc_pca2_clf.best_params_)\n",
    "\n",
    "# Checking best score:\n",
    "print('Best ROC_AUC score: {:.2f}'.format(gbc_pca2_clf.best_score_))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving model:\n",
    "filename = 'gbc_pca2_clf.pkl'\n",
    "pickle.dump(gbc_pca2_clf, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Considering the `roc_auc` metric, this strategy is the worst so far."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.5 XGBoost Classifier and Bayesian Optimization<a name=\"t5\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This time, not only the algorithm will be changed, but also the parameter tuning approach will be changed.\n",
    "\n",
    "Instead of the *Gradient Boosting Classifier*, the `XGBoost Classifier` will be trained on the data. The parameter tuning will be performed by the `BayesSearchCV` algorithm. Instead of simply testing all the parameter combinations, this algorithm test different parameters, given a range of possible values.\n",
    "\n",
    "Once it shows improvement, the algorithm 'explores' deeper the areas that resulted i n better performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining machine learning pipeline:\n",
    "xgbc_ml_pipe_bayes = Pipeline(steps = [\n",
    "    ('preprocessing', preproc),\n",
    "    ('clf', XGBClassifier(random_state = 301))\n",
    "])\n",
    "\n",
    "# Setting parameters to be tested:\n",
    "bayes_search_space = {'clf__booster': Categorical(['gbtree', 'dart']),\n",
    "                      'clf__learning_rate': Real(0.01, 0.3),\n",
    "                      'clf__gamma': Integer(0, 100),\n",
    "                      'clf__min_child_weight': Integer(0, 10),\n",
    "                      'clf__reg_lambda': Integer(1, 100),\n",
    "                      'clf__reg_alpha': Integer(0, 100),\n",
    "                      'clf__tree_method': Categorical(['auto', 'hist']),\n",
    "                      'clf__max_depth': Integer(2, 7)\n",
    "}\n",
    "\n",
    "# Defining function to display scores:\n",
    "def show_score(optim_result):\n",
    "    '''\n",
    "    It shows iteration scores during Bayesian Optimization\n",
    "    '''\n",
    "    # Computing score:\n",
    "    score = xgbc_bayes_clf.best_score_\n",
    "    print('Best ROC_AUC Score:{}'.format(score))\n",
    "\n",
    "    # Early stop:\n",
    "    if score >= 0.81:\n",
    "        print('At least 0.81 ROC_AUC score achieved!')\n",
    "\n",
    "        return True\n",
    "\n",
    "# Grid search + ML pipleine:\n",
    "xgbc_bayes_clf = BayesSearchCV(xgbc_ml_pipe_bayes, bayes_search_space, scoring = 'roc_auc', cv = 5, verbose = 2)\n",
    "\n",
    "# Training model:\n",
    "xgbc_bayes_clf.fit(mailout_train, y, callback = show_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking best parameters:\n",
    "print(xgbc_bayes_clf.best_params_)\n",
    "\n",
    "# Checking best score:\n",
    "print('Best ROC_AUC score: {:.2f}'.format(xgbc_bayes_clf.best_score_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving model:\n",
    "filename = 'xgbc_bayes_clf.pkl'\n",
    "pickle.dump(xgbc_bayes_clf, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This strategy resulted in a score similar to the first one.\n",
    "\n",
    "Since the first one was the best model so far, this new Bayesian Optimization approach will be performed in a few more algorithms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving model:\n",
    "#filename = 'xgbc_bayes_model.pkl'\n",
    "#pickle.dump(xgbc_bayes_clf, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.6 LightGBM and Bayesian Optimization<a name=\"t6\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`LightGBM Classifier` is similar to the *XGBoost Classifier*, but is considered faster. Besides that, it splits its trees leaf-wise, rather than depth or level-wise like most of the other similar algorithms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining machine learning pipeline:\n",
    "lgbm_ml_pipe_bayes = Pipeline(steps = [\n",
    "    ('preprocessing', preproc),\n",
    "    ('clf', LGBMClassifier(random_state = 301))\n",
    "])\n",
    "\n",
    "# Setting parameters to be tested:\n",
    "lgbm_bayes_search_space = {'clf__boosting_type': Categorical(['gbdt', 'dart', 'goss']),\n",
    "                           'clf__num_leaves': Integer(5, 100),\n",
    "                           'clf__max_depth': Integer(2, 200),\n",
    "                           'clf__learning_rate': Real(0.01, 0.5),\n",
    "                           'clf__n_estimators': Integer(100, 500),\n",
    "                           'clf__min_child_samples': Integer(10, 80),\n",
    "                           'clf__reg_alpha': Integer(0, 100),\n",
    "                           'clf__reg_lambda': Integer(0, 100)\n",
    "}\n",
    "\n",
    "# Defining function to display scores:\n",
    "def lgbm_show_score(optim_result):\n",
    "    '''\n",
    "    It shows iteration scores during Bayesian Optimization\n",
    "    '''\n",
    "    # Computing score:\n",
    "    score = lgbm_bayes_clf.best_score_\n",
    "    print('\\nBest ROC_AUC Score: {}.\\n'.format(score))\n",
    "\n",
    "    # Early stop:\n",
    "    if score >= 0.81:\n",
    "        print('At least 0.81 ROC_AUC score achieved!')\n",
    "\n",
    "        return True\n",
    "\n",
    "# Grid search + ML pipleine:\n",
    "lgbm_bayes_clf = BayesSearchCV(lgbm_ml_pipe_bayes, lgbm_bayes_search_space, scoring = 'roc_auc', cv = 5, verbose = 2)\n",
    "\n",
    "# Training model:\n",
    "lgbm_bayes_clf.fit(mailout_train, y, callback = lgbm_show_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking best parameters:\n",
    "print(lgbm_bayes_clf.best_params_)\n",
    "\n",
    "# Checking best score:\n",
    "print('Best ROC_AUC score: {:.2f}'.format(lgbm_bayes_clf.best_score_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving model:\n",
    "filename = 'lgbm_bayes_clf.pkl'\n",
    "pickle.dump(lgbm_bayes_clf, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The *LightGBM Classifier* resulted in a slightly lower score in comparison to the last model, but their performances are comparable.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}