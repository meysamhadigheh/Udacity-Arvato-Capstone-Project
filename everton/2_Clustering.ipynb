{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from .utils import *\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "azdias = pd.read_csv('../data/azdias.csv', sep=';')\n",
    "customers=pd.read_csv('../data/customers.csv',sep=';')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md load needed files\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5 Cluster Analysis<a name=\"cluster\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, unsupervised machine learning techniques will be used in order to identify hidden patterns in the data, clustering the population into different groups, each one composed of people with similar characteristics.\n",
    "\n",
    "With the defined clusters, it will be possible to perform a new comparison between customers and the general population. The difference is that, this time, the comparison won't be performed over one dimension (one variable), but over the different groups created through the effect that all the variables together have on these groups."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5.1 Feature Engineering<a name=\"feateng2\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, there will be one more feature engineering process, this time over `CAMEO_DEU_2015` feature. Since there are 44 different classifications, they will be grouped according to the behavior presented when comparing customers and the general population, following this code:\n",
    "* `0`: *underrepresented classes among clients*;\n",
    "* `1`: *almost equally represented with slight underrepresentation tendency*;\n",
    "* `2`: *almost equally represented with slight overrepresentation tendency*;\n",
    "* `3`: *overrepresented classes among clients*.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Applying transform_cameo_deu on azdias dataframe:\n",
    "azdias = transform_cameo_deu(azdias)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since new features were created during the process, a complementary dictionary will be created to specify the new columns dtypes:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5.2 NaN Values<a name=\"nan\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The approach that will be used to fill in *nan* values is the following:\n",
    "* `Numerical` and `ordinal` variables will have *nan* values replaced with the median of the existing values;\n",
    "\n",
    "* `Categorical` variables will be binarized (One-Hot Encoding), and *nan* values will be indirectly considered."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining function to replace nan values with the median or create dummy variables for following specific distribution:\n",
    "def deal_with_nan_values(df, dict1 = dtypes_dict, dict2 = new_feat_dtypes_dict):\n",
    "    '''\n",
    "    It deals with nan values in two different ways:\n",
    "    * if the feature is defined as 'num' (numerical), it fills nan values with the median value;\n",
    "    * if the feature is defined as 'cat' (categorical) or 'bin' (binary), it creates dummy variables\n",
    "      and deletes the original column.\n",
    "\n",
    "    Inputs:\n",
    "    df: original dataframe;\n",
    "    dict1: main dictionary mapping columns and dtypes;\n",
    "    dict2: auxiliar dictionary mapping new columns and dtypes.\n",
    "\n",
    "    Output:\n",
    "    df: dataframe without nan values.\n",
    "    '''\n",
    "    # Selecting columns with nan values:\n",
    "    nan_cols = list(df.columns[df.isnull().sum() != 0])\n",
    "\n",
    "    # For each column, replace nan values:\n",
    "    for col in nan_cols:\n",
    "\n",
    "        # Verify in which dictionary the column is:\n",
    "        if col in list(dict1.keys()):\n",
    "            dict_ = dict1\n",
    "        else:\n",
    "            dict_ = dict2\n",
    "\n",
    "        # Verify dtype, if numeric:\n",
    "        if dict_[col] == 'num':\n",
    "            # Fill in nan values with median:\n",
    "            df[col].fillna(np.nanmedian(df[col]), inplace = True)\n",
    "\n",
    "        else:\n",
    "            # Delete original column and cocatenate dummy columns:\n",
    "            df = pd.concat([df.drop(col, axis = 1), pd.get_dummies(df[col], prefix = col, \\\n",
    "                                                                   prefix_sep = '_', dummy_na = False)], axis = 1)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5.3 Standardizing Data<a name=\"standard\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Even after the feature selection process, many columns were left to be analyzed. Because of that, Principal Component Analysis will be applied to the data.\n",
    "\n",
    "In order to apply the `PCA` algorithm, the values need to be on the same scale. For that, one function will be defined to fit the model to the data, and another to use the fitted model to transform the data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fitting StandardScaler model:\n",
    "def fit_std_scaler(df):\n",
    "    '''\n",
    "    It uses the dataframe to fit the StandardScaler model.\n",
    "\n",
    "    Input:\n",
    "    df: dataframe used to fit the model.\n",
    "\n",
    "    Output:\n",
    "    cols: list of columns used in the fitting process;\n",
    "    std_model: fitted StandardScaler model.\n",
    "    '''\n",
    "    # Instantiating scaler:\n",
    "    std_scal = StandardScaler()\n",
    "\n",
    "    # Columns used:\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    # Fitting the model:\n",
    "    std_scal.fit(df)\n",
    "\n",
    "    return cols, std_scal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fitting StandardScaler model:\n",
    "def transform_std_scaler(df, cols, scaler):\n",
    "    '''\n",
    "    It uses the fitted StandardScaler model to transform the data.\n",
    "\n",
    "    Input:\n",
    "    df: dataframe used to be transformed;\n",
    "    cols: list of columns to assure dataframe compatibility.\n",
    "    scaler: fitted StandardScaler model.\n",
    "\n",
    "    Output:\n",
    "    df: transformed dataframe.\n",
    "    '''\n",
    "    # Dataframe columns compatibility:\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    # Scaling data:\n",
    "    df = pd.DataFrame(scaler.transform(df), columns = df.columns)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since there are different levels of information, according to the informational spreadsheet, the dimensionality reduction, in this case, the `PCA` technique, will not be applied to the whole dataset at once. Different components will be created for different levels of information.\n",
    "\n",
    "Because of that, the dataset will be split according to their information level, and the transformations will be applied to these subsets.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, all the transformations will be joined together in one function, and then it will be applied to the different levels of information."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining function the apply nan and standard scaler transformations:\n",
    "def transform_data(df, info_level, dic = info_level, cols = None, scaler = None):\n",
    "    '''\n",
    "    It applies the functions that treat nan values and standardize the values.\n",
    "\n",
    "    Inputs:\n",
    "    df: original dataframe;\n",
    "    info_level: string indicating which information level will be treated;\n",
    "    dic: dictionary indicating the information level of each feature.\n",
    "\n",
    "    Output:\n",
    "    df: transformed dataframe with the level of information columns.\n",
    "    '''\n",
    "    # Selecting information level columns:\n",
    "    sel_cols = dic[info_level]\n",
    "    df = df[sel_cols]\n",
    "\n",
    "    # Applying deal_with_nan_values function:\n",
    "    df = deal_with_nan_values(df)\n",
    "\n",
    "    # If scaler is not defined, create scaler:\n",
    "    if scaler == None:\n",
    "        # Fitting StandardScaler model:\n",
    "        cols, scaler = fit_std_scaler(df)\n",
    "\n",
    "        # Standardizing data:\n",
    "        df = transform_std_scaler(df, cols, scaler)\n",
    "\n",
    "        return df, cols, scaler\n",
    "\n",
    "    else:\n",
    "        # Standardizing data:\n",
    "        df = transform_std_scaler(df, cols, scaler)\n",
    "\n",
    "        return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Applying data transformation on:\n",
    "# 'person' info level:\n",
    "azdias_pers, pers_cols, pers_scaler = transform_data(azdias, info_level = 'person')\n",
    "azdias_pers.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "azdias_pers.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 'household' info level:\n",
    "azdias_hh, hh_cols, hh_scaler = transform_data(azdias, info_level = 'household')\n",
    "azdias_hh.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "azdias_hh.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 'microcell' info level:\n",
    "azdias_mic, mic_cols, mic_scaler = transform_data(azdias, info_level = 'microcell')\n",
    "azdias_mic.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "azdias_mic.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 'macrocell' info level:\n",
    "azdias_mac, mac_cols, mac_scaler = transform_data(azdias, info_level = 'macrocell')\n",
    "azdias_mac.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "azdias_mac.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 'community' info level:\n",
    "azdias_com, com_cols, com_scaler = transform_data(azdias, info_level = 'community')\n",
    "azdias_com.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "azdias_com.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Information Level Summary:')\n",
    "print('--------------------------')\n",
    "print('Personal level has {} features.'.format(azdias_pers.shape[1]))\n",
    "print('Household level has {} features.'.format(azdias_hh.shape[1]))\n",
    "print('Microcell level has {} features.'.format(azdias_mic.shape[1]))\n",
    "print('Macrocell level has {} features.'.format(azdias_mac.shape[1]))\n",
    "print('Community level has {} features.'.format(azdias_com.shape[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Considering the number of features after the transformations, `PCA` will be applied only on the first four levels of information:\n",
    "* Personal;\n",
    "* Household;\n",
    "* Microcell;\n",
    "* Macrocell.\n",
    "\n",
    "The Community level has only three features, and because of that, they will be kept without further transformations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5.4 Dimensionality Reduction<a name=\"pca\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the data is treated and standardized, before actually applying the dimensionality reduction, `PCA` will be applied with standard parameters in order to decide the number of components to keep.\n",
    "\n",
    "To make the number of components decision, a *scree plot* will be created for each level of information data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining function to create scree plot to decide how many components to keep:\n",
    "def create_scree_plot(pca):\n",
    "    '''\n",
    "    It creates a scree plot for the fitted principal component analysis in order to define the number\n",
    "    of components to keep.\n",
    "\n",
    "    Input:\n",
    "    pca: principal component analysis object fitted to data.\n",
    "    '''\n",
    "    # Getting the variance's percentage explained by each component:\n",
    "    perc_vars = pca.explained_variance_ratio_\n",
    "\n",
    "    # Getting cumulative values of the variance's percentage:\n",
    "    cum_perc_vars = np.cumsum(perc_vars)\n",
    "\n",
    "    # Defining the number of components:\n",
    "    n_comp = len(pca.explained_variance_ratio_)\n",
    "\n",
    "    # Defining the index for each component:\n",
    "    index = np.arange(n_comp)\n",
    "\n",
    "    # Creating scree plot:\n",
    "    sns.set_theme(style = \"whitegrid\", font_scale = 1.1)\n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (13, 5))\n",
    "\n",
    "    # Drawing lineplot:\n",
    "    sns.lineplot(x = index, y = cum_perc_vars,\n",
    "                 markers = True, color = 'black').set(xlabel = 'Number of Principal Components',\n",
    "                                                      ylabel = 'Explained Variance',\n",
    "                                                      title = \"Scree Plot - Cumulative Explained Variance\",\n",
    "                                                      ylim = (0,1.02))\n",
    "    sns.despine()\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purpose is to retain most of the data variability, and the few components as possible, simplifying the resulted data.\n",
    "\n",
    "Since it's a client segmentation problem, the assumption is that the `person` information level must be more important than the `household` information level, which is more important than the `microcell` and the `macrocell` information levels, being the last one the more general in this scale.\n",
    "\n",
    "Because of that, when deciding the number of components to keep, more important levels will have the number of components necessary to explain about 60% of the variance, while less important levels will be allowed to have a lower explained variance rate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PERSON level:\n",
    "pca = PCA()\n",
    "pers_pca = pca.fit(azdias_pers)\n",
    "\n",
    "# Getting the scree plot:\n",
    "create_scree_plot(pers_pca)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to keep about 60% of the explained variance, in the `person` level **30 components** will be kept."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# HOUSEHOLD level:\n",
    "hh_pca = pca.fit(azdias_hh)\n",
    "\n",
    "# Getting the scree plot:\n",
    "create_scree_plot(hh_pca)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the `household` level, **20 components** explain over 50% of the data variance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MICROCELL level:\n",
    "mic_pca = pca.fit(azdias_mic)\n",
    "\n",
    "# Getting the scree plot:\n",
    "create_scree_plot(mic_pca)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MACROCELL level:\n",
    "mac_pca = pca.fit(azdias_mac)\n",
    "\n",
    "# Getting the scree plot:\n",
    "create_scree_plot(mac_pca)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because of the scale of importance assumed before, `microcell` and `macrocell` levels will be allowed to have their rate of explained variance between 40 and 50%, keeping **10 components** for each one of them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>Applying PCA</h4>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#____________________________________________________________\n",
    "# PERSON:\n",
    "\n",
    "# Instantiating pca object:\n",
    "pca_pers = PCA(n_components = 30, random_state = 101)\n",
    "\n",
    "# Fitting pca and applyting transformation on azdias dataset:\n",
    "azdias_pers_pca = pca_pers.fit_transform(azdias_pers)\n",
    "\n",
    "#____________________________________________________________\n",
    "# HOUSEHOLD:\n",
    "\n",
    "# Instantiating pca object:\n",
    "pca_hh = PCA(n_components = 20, random_state = 201)\n",
    "\n",
    "# Fitting pca and applyting transformation on azdias dataset:\n",
    "azdias_hh_pca = pca_hh.fit_transform(azdias_hh)\n",
    "\n",
    "#____________________________________________________________\n",
    "# MICROCELL:\n",
    "\n",
    "# Instantiating pca object:\n",
    "pca_mic = PCA(n_components = 10, random_state = 301)\n",
    "\n",
    "# Fitting pca and applyting transformation on azdias dataset:\n",
    "azdias_mic_pca = pca_mic.fit_transform(azdias_mic)\n",
    "\n",
    "#____________________________________________________________\n",
    "# MACROCELL:\n",
    "\n",
    "# Instantiating pca object:\n",
    "pca_mac = PCA(n_components = 10, random_state = 401)\n",
    "\n",
    "# Fitting pca and applyting transformation on azdias dataset:\n",
    "azdias_mac_pca = pca_mac.fit_transform(azdias_mac)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Column names for general dataframe:\n",
    "person_cols = ['pers01', 'pers02', 'pers03', 'pers04', 'pers05', 'pers06', 'pers07', 'pers08', 'pers09', 'pers10', 'pers11',\n",
    "               'pers12', 'pers13', 'pers14', 'pers15', 'pers16', 'pers17', 'pers18', 'pers19', 'pers20', 'pers21', 'pers22',\n",
    "               'pers23', 'pers24', 'pers25', 'pers26', 'pers27', 'pers28', 'pers29', 'pers30']\n",
    "\n",
    "household_cols = ['hh01', 'hh02', 'hh03', 'hh04', 'hh05', 'hh06', 'hh07', 'hh08', 'hh09', 'hh10', 'hh11', 'hh12', 'hh13',\n",
    "                  'hh14', 'hh15', 'hh16', 'hh17', 'hh18', 'hh19', 'hh20']\n",
    "\n",
    "microcell_cols = ['mic01', 'mic02', 'mic03', 'mic04', 'mic05', 'mic06', 'mic07', 'mic08', 'mic09', 'mic10']\n",
    "\n",
    "macrocell_cols = ['mac01', 'mac02', 'mac03', 'mac04', 'mac05', 'mac06', 'mac07', 'mac08', 'mac09', 'mac10']\n",
    "\n",
    "# Concatenating pca dataframes together, also including the 3 features related to the community information level:\n",
    "gen_azdias = pd.concat([pd.DataFrame(azdias_pers_pca, columns = person_cols),\n",
    "                        pd.DataFrame(azdias_hh_pca, columns = household_cols),\n",
    "                        pd.DataFrame(azdias_mic_pca, columns = microcell_cols),\n",
    "                        pd.DataFrame(azdias_mac_pca, columns = macrocell_cols),\n",
    "                        azdias_com], \\\n",
    "                       axis = 1)\n",
    "\n",
    "gen_azdias.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To better understand how the original features compose these components created through `PCA`, a function will be defined to return the most important features for each component:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining function to show most important features for each component:\n",
    "def analyze_component(component, top = 3):\n",
    "    '''\n",
    "    It returns the most important features related to the component.\n",
    "\n",
    "    Inputs:\n",
    "    component: integer indicating the number of the component to be analyzed;\n",
    "    top: integer indicating the number of top features to return;\n",
    "    '''\n",
    "    # Checking on to which level of informations the component relates to:\n",
    "    # PERSON component:\n",
    "    if component in range(30):\n",
    "        print('\\nPERSON Component:')\n",
    "        print('-----------------')\n",
    "        pca = pca_pers\n",
    "        features = list(azdias_pers.columns)\n",
    "\n",
    "    # HOUSEHOLD component:\n",
    "    elif component in range(30, 50):\n",
    "        print('\\nHOUSEHOLD Component:')\n",
    "        print('--------------------')\n",
    "        pca = pca_hh\n",
    "        features = list(azdias_hh.columns)\n",
    "        component = component - 30\n",
    "\n",
    "    # MICROCELL component:\n",
    "    elif component in range(50, 60):\n",
    "        print('\\nMICROCELL Component:')\n",
    "        print('--------------------')\n",
    "        pca = pca_mic\n",
    "        features = list(azdias_mic.columns)\n",
    "        component = component - 50\n",
    "\n",
    "    # MACROCELL component:\n",
    "    elif component in range(60, 70):\n",
    "        print('\\nMACROCELL Component:')\n",
    "        print('--------------------')\n",
    "        pca = pca_mac\n",
    "        features = list(azdias_mac.columns)\n",
    "        component = component - 60\n",
    "\n",
    "    else:\n",
    "        return(print('Component out of range (0-69)'))\n",
    "\n",
    "\n",
    "    # Extracting features' weight on the desired component:\n",
    "    weights = list(pca.components_[component])\n",
    "\n",
    "    # Getting the index of ordered weights:\n",
    "    ordered_idx = np.argsort(weights)[::-1]\n",
    "\n",
    "    # Getting top indexes:\n",
    "    top_pos_idx = ordered_idx[:top]\n",
    "    top_neg_idx = ordered_idx[-top:]\n",
    "\n",
    "    # Printing top positive weights on the component:\n",
    "    print('\\nComponent {}'.format(component))\n",
    "    print('------------')\n",
    "    print('\\n* Top {} Positive Weights:'.format(top))\n",
    "    print('--------------------------')\n",
    "    for i in top_pos_idx:\n",
    "        print('{}: {:.3f}'.format(features[i], weights[i]))\n",
    "\n",
    "    # Printing top negative weights on the component:\n",
    "    print('\\n* Top {} Negative Weights:'.format(top))\n",
    "    print('--------------------------')\n",
    "    for j in top_neg_idx[::-1]:\n",
    "        print('{}: {:.3f}'.format(features[j], weights[j]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Testing analyze_component:\n",
    "analyze_component(0, top = 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately, there was no explanation provided for the feature `CJT_TYP_6` (it's only known to be related to *customer journey typology*). However, as an example, the **Component 0** related to the **person** information level will be interpreted without this specific feature.\n",
    "\n",
    "`ALTERSKATEGORIE_GROB` relates to age classification through prename analysis, where higher values represent higher ages, indicating that these components represent elder people. This aspect can be reinforced when analyzing the highest negative weight related to the `YOUTH_DECADE` component.\n",
    "\n",
    "`YOUTH_DECADE` indicates in which decade the person lived his/her youth period. In other words, the lower the decade, the elder the person is. Basically, these two variables represent the same information, but their values are in opposite directions, confirming that this component represents elder people.\n",
    "\n",
    "It can also be seen that `SEMIO_ERL` and `SEMIO_LUST` are important features that positively represent this component. `SEMIO_ERL` describes if the person is eventful oriented while `SEMIO_LUST`indicates if the person is sensual minded. Higher values indicate lower affinity with that specific characteristic (*1 - highest affinity*, *7 - lowest affinity*). It tells that this component represents elder people that are *not* eventful oriented, nor sensual minded.\n",
    "\n",
    "On the oher hand, `SEMIO_TRADV` and `SEMIO_REL` represent important aspects in the opposite direction, indicating that the component represents people that are traditional-minded and religious. At the same time, they have a rational mind (`SEMIO_RAT`).\n",
    "\n",
    "`RETOURTYP_BK_S_5.0` indicates that the return type of these people is classified as *determined minimal-returner*, and the `FINANZ_ALENGER` indicates a high correlation to the *investor* financial typology, maybe giving a hint that these people may also be associated with higher incomes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5.5 Defining the Number of Clusters<a name=\"nclusters\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before classifying the data in different clusters, it's necessary to find the optimal number of clusters. For that, the *Elbow Method* will be used:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining function to create Elbow Method Visualization\n",
    "def apply_elbow_method(df = gen_azdias):\n",
    "    '''\n",
    "    It returns a visualization that shows the sum of squared distances of samples to their closest cluster\n",
    "    center for each attempt of number of clusters.\n",
    "\n",
    "    Input:\n",
    "    df: dataframe on which cluster analysis will be performed.\n",
    "    '''\n",
    "    # Defining number of clusters attempts:\n",
    "    n_cluster = range(2, 32, 2)\n",
    "\n",
    "    # Creating inertia list:\n",
    "    wcss = list()\n",
    "\n",
    "    # Perform K-Means for each attempt and extract its 'inertia' (sum of quared distances to their closest cluster):\n",
    "    for n in n_cluster:\n",
    "        kmeans = KMeans(n_clusters = n)\n",
    "        kmeans.fit(df)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    # Creating the visualization:\n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (13, 5))\n",
    "\n",
    "    # Drawing lineplot:\n",
    "    sns.lineplot(x = n_cluster, y = wcss,\n",
    "                 markers = True, color = 'black').set(xlabel = 'Number of Clusters',\n",
    "                                                      ylabel = 'Sum of Squared Distances',\n",
    "                                                      title = \"Elbow Method - Optimizing Number of Clusters Choice\")\n",
    "    sns.despine()\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking Elbow visualization:\n",
    "apply_elbow_method()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately, there's no clear elbow indicating which would be the best number of clusters to be chosen. However, it's possible to see that the most expressive change in the inclination happens between 15 and 20 clusters.\n",
    "\n",
    "With that, the clustering process will be built considering **18 clusters**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5.6 Applying Transformations on Customer Data<a name=\"transcustomer\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before proceeding to the clustering process, data transformation will be applied to the customers data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CAMEO_DEU feature engineering:\n",
    "customers = transform_cameo_deu(customers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Applying data transformation on:\n",
    "# 'person' info level:\n",
    "customers_pers = transform_data(customers, info_level = 'person', cols = pers_cols, scaler = pers_scaler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 'household' info level:\n",
    "customers_hh = transform_data(customers, info_level = 'household', cols = hh_cols, scaler = hh_scaler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 'microcell' info level:\n",
    "customers_mic = transform_data(customers, info_level = 'microcell', cols = mic_cols, scaler = mic_scaler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 'macrocell' info level:\n",
    "customers_mac = transform_data(customers, info_level = 'macrocell', cols = mac_cols, scaler = mac_scaler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 'community' info level:\n",
    "customers_com = transform_data(customers, info_level = 'community', cols = com_cols, scaler = com_scaler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PCA transformation:\n",
    "\n",
    "#_______________________________________________________\n",
    "# PERSON:\n",
    "\n",
    "# Applyting transformation on customers dataset:\n",
    "customers_pers_pca = pca_pers.transform(customers_pers)\n",
    "\n",
    "#_______________________________________________________\n",
    "# HOUSEHOLD:\n",
    "\n",
    "# Applyting transformation on customers dataset:\n",
    "customers_hh_pca = pca_hh.transform(customers_hh)\n",
    "\n",
    "#_______________________________________________________\n",
    "# MICROCELL:\n",
    "\n",
    "# Applyting transformation on customers dataset:\n",
    "customers_mic_pca = pca_mic.transform(customers_mic)\n",
    "\n",
    "#_______________________________________________________\n",
    "# MACROCELL:\n",
    "\n",
    "# Applyting transformation on customers dataset:\n",
    "customers_mac_pca = pca_mac.transform(customers_mac)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Concatenating pca dataframes together, also including the 3 features related to the community information level:\n",
    "gen_customers = pd.concat([pd.DataFrame(customers_pers_pca, columns = person_cols),\n",
    "                           pd.DataFrame(customers_hh_pca, columns = household_cols),\n",
    "                           pd.DataFrame(customers_mic_pca, columns = microcell_cols),\n",
    "                           pd.DataFrame(customers_mac_pca, columns = macrocell_cols),\n",
    "                           customers_com], \\\n",
    "                          axis = 1)\n",
    "\n",
    "gen_customers.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5.7 Clustering<a name=\"clustering\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training KMeans algorithm and making cluster predictions:\n",
    "# Instatiating KMeans algorithm for n clusters:\n",
    "kmeans = KMeans(n_clusters = 18, random_state = 101)\n",
    "\n",
    "# Fitting to df1 data:\n",
    "kmeans.fit(gen_azdias)\n",
    "\n",
    "# Predicting clusters on df1:\n",
    "azdias_clusters = kmeans.predict(gen_azdias)\n",
    "\n",
    "# Predicting clusters on df2:\n",
    "customers_clusters = kmeans.predict(gen_customers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5.8 Evaluating Clusters<a name=\"clustereval\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once each observation was assigned to its correspondent cluster, the job now is to check which clusters are proportionally more representative among clients than in the general population. The same for the clusters that happen more frequently in the general population in comparison to the customers' group.\n",
    "\n",
    "With that, it will be possible to understand the different combinations of features that result in a person being more likely to become a client or the other way around."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining a function to create a comparison barplot:\n",
    "def compare_cluster_occurance(customers = customers_clusters, general = azdias_clusters):\n",
    "    '''\n",
    "    It creates a bor plot comparing the percentages related to each one the clusters borh for customers and\n",
    "    the general population.\n",
    "\n",
    "    Inputs:\n",
    "    customers: array containing the assigned cluster for each customer observation;\n",
    "    general: rray containing the assigned cluster for each general population observation.\n",
    "    '''\n",
    "    # Counting cluster occurences in customer population:\n",
    "    customer_cluster_occ = np.bincount(customers)\n",
    "\n",
    "    # Counting cluster occurences in general population:\n",
    "    gen_pop_cluster_occ = np.bincount(general)\n",
    "\n",
    "    # Creating dataframe with cluster proportional counting:\n",
    "    cluster_df = pd.DataFrame(columns = ['general_population', 'customers'])\n",
    "    n_pop = np.sum(gen_pop_cluster_occ)\n",
    "    n_cust = np.sum(customer_cluster_occ)\n",
    "    cluster_df.general_population = [(count / n_pop) * 100 for count in gen_pop_cluster_occ]\n",
    "    cluster_df.customers = [(count / n_cust) * 100 for count in customer_cluster_occ]\n",
    "    cluster_df['Cluster'] = cluster_df.index.values\n",
    "\n",
    "    # Transforming df to melted version:\n",
    "    melted_df = pd.melt(cluster_df, id_vars = ['Cluster'] , value_vars = ['general_population', 'customers'], \\\n",
    "                        var_name = 'Group', value_name ='Percentage')\n",
    "\n",
    "    palette = {'customers': 'darkcyan', 'general_population': 'springgreen'}\n",
    "\n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (16.5, 5))\n",
    "    sns.barplot(x = 'Cluster', y = 'Percentage', hue = 'Group', data = melted_df, palette = palette, ax = ax).set(\n",
    "        title = 'Comparison Between Customers and General Population - Clusters')\n",
    "    sns.despine(left=True, top = True)\n",
    "    fig.show()\n",
    "\n",
    "    # Computing percentage differences:\n",
    "    perc_diff = [((cluster_df.customers[i] - cluster_df.general_population[i]) / cluster_df.general_population[i]) * 100 \\\n",
    "                 for i in range(cluster_df.shape[0])]\n",
    "    # Getting ordered index:\n",
    "    ordered_idx = np.argsort(perc_diff)[::-1]\n",
    "\n",
    "    # Printing top 5 customers' clusters with highest representativity in comparison to general population:\n",
    "    print('Top 5 Overrepresented Clusters among Customers:')\n",
    "    print('-----------------------------------------------\\n')\n",
    "    for i in ordered_idx[:5]:\n",
    "        print('Cluster {}: increased {:.1f} %'.format(i, perc_diff[i]))\n",
    "\n",
    "    # Printing top 5 customers' clusters with lowest representativity in comparison to general population:\n",
    "    print('\\nTop 5 Underrepresented Clusters among Customers:')\n",
    "    print('-----------------------------------------------\\n')\n",
    "    for i in ordered_idx[-5:][::-1]:\n",
    "        print('Cluster {}: decreased {:.1f} %'.format(i, perc_diff[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compare_cluster_occurance()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To better understand what these clusters represent, `cluster center` will provide the most important components related to that specific cluster, and then it will be possible to go back to those specific components to understand the features that better represent them.\n",
    "\n",
    "That way, it will be possible to develop a cluster overview."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating a function that, given one specific cluster, returns the most important features associated with it:\n",
    "def cluster_most_important_features(cluster, kmeans = kmeans, top_comp = 3):\n",
    "    '''\n",
    "    Given a cluster, it searches for the most important components, and through them, search for\n",
    "    the most important features related to those components.\n",
    "\n",
    "    Input:\n",
    "    cluster: integer indicating the target cluster (0-17);\n",
    "    kmeans: trained kmeans model\n",
    "    top_com: number of most important components to be analyzed.\n",
    "    '''\n",
    "    # Getting cluster center:\n",
    "    cluster_centers = kmeans.cluster_centers_[cluster]\n",
    "\n",
    "    # Getting more important components:\n",
    "    pos_idx = np.argsort(cluster_centers)[::-1]\n",
    "    pos_idx = pos_idx[:top_comp]\n",
    "\n",
    "    # Analyzing components and extracting their most important features:\n",
    "    print('Principal Components with Highest Impact on Cluster {}:'.format(cluster))\n",
    "    print('--------------------------------------------------------')\n",
    "    for comp in pos_idx:\n",
    "        analyze_component(comp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5.8.1 Overrepresented Clusters<a name=\"over\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Cluster 2 - GREEN DREAMERS**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Analyzing Cluster 2:\n",
    "cluster_most_important_features(2, top_comp = 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`CLUSTER 2` is the one with the highest overrepresentation, being proportionally over 205% more representative in customers than in the general population.\n",
    "\n",
    "Analyzing the most important components, that's how this cluster could be described:\n",
    "* their vacation habits can be classified as *nature fans*, already emphasizing the green tendency as an important aspect present in people represented by this cluster;\n",
    "\n",
    "* the green aspect is reinforced with the *avant-garde* aspect, relating this cluster to people with the avant-garde mindset, which also relates them to the *green avant-garde* movements;\n",
    "\n",
    "* they don't seem to be familiar minded;\n",
    "\n",
    "* this cluster is representing mostly men that are not single, not critical-minded but are dreamers and social-minded."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Cluster 9 - HIGH-SOCIETY TRADITIONAL ELDERS**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Analyzing Cluster 9:\n",
    "cluster_most_important_features(9, top_comp = 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`CLUSTER 9` is also about 205% more representative in customers than in the general population.\n",
    "\n",
    "Analyzing the most important components, that's how this cluster could be described:\n",
    "* they are in advanced life stages, being represented as families or multiperson households, and also related to higher incomes;\n",
    "\n",
    "* their social status can be classified as independents, house owners and top-earners;\n",
    "\n",
    "* they are not dreamers, nor eventful oriented, but they can be represented as being critical and traditional-minded, as well as religious;\n",
    "\n",
    "* the cluster describes elder people."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Cluster 4 - EMPTY NEST, FULL WALLET**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Analyzing Cluster 4:\n",
    "cluster_most_important_features(4, top_comp = 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`CLUSTER 4` is over 100% more representative in customers than in the general population.\n",
    "\n",
    "Analyzing the most important components, that's how this cluster could be described:\n",
    "* it represents mostly average to high-income couples;\n",
    "\n",
    "* it could also represent house owner couples and top earner-couples of higher age;\n",
    "\n",
    "* the third component brings aspects seen before, as being  traditional-minded, and religious, but not eventful oriented;\n",
    "\n",
    "* it's possible to say that some aspects seen before are now applied mostly to couples with high-incomes from different ages, and also mature and top earner-couples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5.8.2 Underrepresented Clusters<a name=\"under\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Cluster 0 - LESS AFFORTUNATE BEGINNERS**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Analyzing Cluster 0:\n",
    "cluster_most_important_features(0, top_comp = 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`CLUSTER 0` is 95% less representative in customers than in the general population.\n",
    "\n",
    "Analyzing the most important components, that's how this cluster could be described:\n",
    "* it represents people that could be classified as less affluent or poorer, mostly indicating pre-family couples and singles or young couples with children;\n",
    "\n",
    "* their household incomes can be considered as lower or very low incomes, and their insurance typology is mostly classified as *social safety-driven*;\n",
    "\n",
    "* in terms of health and shopping typologies, they could be described as *sanitary affine* and *demanding shoppers*;\n",
    "\n",
    "* the microcell component indicates areas with a high share of cares built before 1994 that had two or more pre-owners, creating the picture of people with lower incomes that live in poorer areas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Cluster 16 - MULTI-GENERATION MONEY SAVERS**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Analyzing Cluster 9:\n",
    "cluster_most_important_features(16, top_comp = 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`CLUSTER 9` is also about 95% less representative in customers than in the general population.\n",
    "\n",
    "Analyzing the most important components, that's how this cluster could be described:\n",
    "* this cluster describes people with low-income and average earners of higher age from multiperson households, and families classified as *two-generation household*;\n",
    "\n",
    "* there is also a strong aspect of singles with low-incomes or average-earners families, and also average earners of younger age from multiperson households. This profile has a tendency of being related to people living in the Westside;\n",
    "\n",
    "* their financial typology to be classified as *low financial interest* or *money-savers*."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Cluster 14 - SECOND-HAND CAR CELLS**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Analyzing Cluster 14:\n",
    "cluster_most_important_features(14, top_comp = 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`CLUSTER 14` is about 83% less representative in customers than in the general population.\n",
    "\n",
    "Analyzing the most important components, that's how this cluster could be described:\n",
    "* it represents people that could be classified as less affluent or poorer, mostly indicating pre-family couples and singles or young couples with children;\n",
    "\n",
    "* the microcell component indicates areas with a high share of cares built before 1994 that had two or more pre-owners, creating the picture of people with lower incomes that live in poorer areas;\n",
    "\n",
    "* it also describes areas with a high share of family houses, with a very low share of cars per household, and also close to city centers;\n",
    "\n",
    "* most of these aspects were already highlighted in other clusters, but in this case, it seems to emphasize more the micro and macrocells aspects than the ones related to the person.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}